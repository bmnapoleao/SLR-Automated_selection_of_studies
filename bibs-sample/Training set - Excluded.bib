@book{10.5555/1121729,
	title        = {A Guide To The Project Management Body Of Knowledge (PMBOK Guides)},
	year         = 2004,
	publisher    = {Project Management Institute},
	isbn         = {193069945X},
	abstract     = {Its hard to imagine a time when A Guide to the Project Management Body of Knowledge (PMBOK® Guide) wasnt around. Yet, just twenty years ago, PMI volunteers first sat down to distill the project management body of knowledge. Their hard work eventually became the PMBOK® Guide, now considered one of the most essential tools in the profession and is the de facto global standard for the industry. With more than a million copies of the PMBOK® Guide2000 Edition in use, PMI has received numerous positive comments and suggestions for improvements. Methodical updates occur on a four-year cycle to ensure PMIs commitment to continually improve and revise the information contained in this essential reference manual. Users will find a number of changes when they upgrade from the PMBOK® Guide2000 Edition. One of the most important changes is the criteria for included information, which evolved from generally accepted on most projects, most of the time to generally recognized as good practice on most projects, most of the time. Unique to the PMBOK® GuideThird Edition is the increased clarity and emphasis on processes, including highlighting that the five Process Groups are the key to the management of projects. As the updated official standard of the worlds leading project management organization, PMBOK® Guide Third Edition is an essential reference tool for every project management practitioners library.}
}
@inproceedings{10.1145/1297846.1297941,
	title        = {Continuing Professional Development by Practitioner Integrated Learning},
	author       = {Borjesson, Anna and Pareto, Lars and Snis, Ulrika Lundh and Staron, Miroslaw},
	year         = 2007,
	booktitle    = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
	location     = {Montreal, Quebec, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {OOPSLA '07},
	pages        = {897–907},
	doi          = {10.1145/1297846.1297941},
	isbn         = 9781595938657,
	url          = {https://doi.org/10.1145/1297846.1297941},
	abstract     = {To prevent skilled professionals from being phased out or forced into professions for which they are not talented, organized forms of lifelong learning are needed. Continuing professional development is an approach supporting lifelong learning. This approach is however criticized for being expensive and not providing the necessary knowledge. In response to this, we have executed a study in order to understand how universities can effectively support continuous professional development. By involving industry professionals as participants in university courses using problem based learning, we have designed what we call Practitioner Integrated Learning (PIL). This learning approach has shown positive effects in terms of level of learning, realism, knowledge diffusion, study load and costs. We present a 15-months action research project integrating 16 industry managers and 16 university students in a continuing professional development effort. Based on this study, we argue that PIL is a learning approach that effectively supports continuing professional development.},
	numpages     = 11,
	keywords     = {continuing professional development, PBL, lifelong learning, practitioner integrated learning, action research, problem based learning}
}
@inproceedings{1191351,
	title        = {On a partnership between software industry and academia},
	author       = {Kornecki, A.J. and Khajenoori, S. and Gluch, D. and Kameli, N.},
	year         = 2003,
	month        = {March},
	booktitle    = {Proceedings 16th Conference on Software Engineering Education and Training, 2003. (CSEE&T 2003).},
	volume       = {},
	number       = {},
	pages        = {60--69},
	doi          = {10.1109/CSEE.2003.1191351},
	issn         = {1093-0175},
	abstract     = {This paper discusses a role for industry in software engineering education, specifically presenting a university-industry partnership between the Cardiac Rhythm Management (CRM) organization at the Guidant Corporation and Embry-Riddle Aeronautical University (ERAU). The focus of the partnership is technology transition. The partnership involves fostering students' professional development, providing students experience solving realworld problems, and exploring modern directions of software engineering. The critical component of the partnership is a student-oriented research laboratory. After discussing the background and history of the project, we focus on the partnership's accomplishments. These include facilitating the transition of graduates from student to employee by developing in them extended software engineering skills and in-depth understanding of the application domain.},
	keywords     = {}
}
@inproceedings{6928785,
	title        = {Network Analysis of a Large Scale Open Source Project},
	author       = {Orucevic-Alagic, Alma and Höst, Martin},
	year         = 2014,
	month        = {Aug},
	booktitle    = {2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
	volume       = {},
	number       = {},
	pages        = {25--29},
	doi          = {10.1109/SEAA.2014.50},
	issn         = {2376-9505},
	abstract     = {One way to understand the structure of an open source community is by applying network analysis to its source code repositories. In this paper a new method for the analysis of committers' networks is proposed. The method deals with directed and weighted committers' networks. The method is then applied to the Android open source project. The analysis results show how a large, company sponsored, and industry backed open source project, i.e. An open source project with the majority of the community members affiliated with the industry, is structured. In particular, it shows that the involvement of an entire industry eco system within a company sponsored open source project does not imply more equal distribution of the participating community members' influences in terms of committers' networks.},
	keywords     = {}
}
@article{article,
	title        = {Design Science in Information Systems Research},
	author       = {Hevner, Alan and R, Alan and March, Salvatore and T, Salvatore and Park, and Park, Jinsoo and Ram, and Sudha,},
	year         = 2004,
	month        = {03},
	journal      = {Management Information Systems Quarterly},
	volume       = 28,
	pages        = {75-},
	abstract     = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
	keywords     = {nformation Systems research meth-odologies, design science, design artifact, busi-ness environment, technology infrastructure,search strategies, experimental methods, creativity}
}
@inproceedings{10.1145/2494091.2494110,
	title        = {Some like It Hot: Automating an Electric Kettle Using PalCom},
	author       = {Magnusson, Boris and Johnsson, Bj\"{o}rn A.},
	year         = 2013,
	booktitle    = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
	location     = {Zurich, Switzerland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {UbiComp '13 Adjunct},
	pages        = {63–66},
	doi          = {10.1145/2494091.2494110},
	isbn         = 9781450322157,
	url          = {https://doi.org/10.1145/2494091.2494110},
	abstract     = {In this demo we will show how devices from different vendors, using different protocols, can be combined and made to work together without detailed low-level programming by the user. The small example we have chosen uses a radio-controlled power socket from one vendor and a temperature sensor from another vendor. We use these to create a remotely controlled electric kettle, which keeps the water at the point of boiling, ready to make tea at any time. We also show how we very easy can use a mobile phone for remote control and monitoring of the kettle. It is all built with a simple-to-use graphical user interface offered by the PalCom middleware, and will be modified as part of the demo.},
	numpages     = 4,
	keywords     = {palcom, ad-hoc composition, services, middleware, pervasive systems, devices}
}
@inproceedings{inproceedings,
	title        = {Factors Influencing Industrial Practices of Software Architecture Evaluation: An Empirical Investigation},
	author       = {Ali Babar, Muhammad and Bass, Len and Gorton, Ian},
	year         = 2007,
	month        = {07},
	volume       = 4880,
	pages        = {90--107},
	doi          = {10.1007/978-3-540-77619-2_6},
	abstract     = {To support software architecture evaluation practices, several efforts have been made to provide a basis for comparing and assessing evaluation methods, document various best practices, and report the factors that may influence industrial practices. However, there has been no study to explore the experiences and perceptions of architects for determining the factors that influence architecture evaluation practices in a wide range of organizations. Hence, there is little empirically founded knowledge available on the factors that influence the industrial practices of software architecture evaluation. The goal of this paper is to report the results of an empirical study aimed at gaining an understanding of different factors involved in evaluating architectures in industry. The results of this study shed light on the factors that influence architecture evaluation practices based on the experiences and perception of architects who regularly evaluate architectures of various sizes of applications. It also discusses some of the strategies that practitioners apply to deal with the influence of the identified factors.},
	keywords     = {Focus Group, Quality Attribute, Software Architecture, Focus Group Session, Governance Framework}
}
@article{GENCEL20133091,
	title        = {A decision support framework for metrics selection in goal-based measurement programs: GQM-DSFMS},
	author       = {Cigdem Gencel and Kai Petersen and Aftab Ahmad Mughal and Muhammad Imran Iqbal},
	year         = 2013,
	journal      = {Journal of Systems and Software},
	volume       = 86,
	number       = 12,
	pages        = {3091--3108},
	doi          = {https://doi.org/10.1016/j.jss.2013.07.022},
	issn         = {0164-1212},
	url          = {https://www.sciencedirect.com/science/article/pii/S0164121213001726},
	keywords     = {Software measurement program, Goal based measurement, Goal Question Metric, GQM, Decision support, Optimization, Prioritization},
	abstract     = {Software organizations face challenges in managing and sustaining their measurement programs over time. The complexity of measurement programs increase with exploding number of goals and metrics to collect. At the same time, organizations usually have limited budget and resources for metrics collection. It has been recognized for quite a while that there is the need for prioritizing goals, which then ought to drive the selection of metrics. On the other hand, the dynamic nature of the organizations requires measurement programs to adapt to the changes in the stakeholders, their goals, information needs and priorities. Therefore, it is crucial for organizations to use structured approaches that provide transparency, traceability and guidance in choosing an optimum set of metrics that would address the highest priority information needs considering limited resources. This paper proposes a decision support framework for metrics selection (DSFMS) which is built upon the widely used Goal Question Metric (GQM) approach. The core of the framework includes an iterative goal-based metrics selection process incorporating decision making mechanisms in metrics selection, a pre-defined Attributes/Metrics Repository, and a Traceability Model among GQM elements. We also discuss alternative prioritization and optimization techniques for organizations to tailor the framework according to their needs. The evaluation of the GQM-DSFMS framework was done through a case study in a CMMI Level 3 software company.}
}
@article{BACA20132411,
	title        = {Countermeasure graphs for software security risk assessment: An action research},
	author       = {Dejan Baca and Kai Petersen},
	year         = 2013,
	journal      = {Journal of Systems and Software},
	volume       = 86,
	number       = 9,
	pages        = {2411--2428},
	doi          = {https://doi.org/10.1016/j.jss.2013.04.023},
	issn         = {0164-1212},
	url          = {https://www.sciencedirect.com/science/article/pii/S0164121213001027},
	keywords     = {Software security, Risk analysis, Countermeasure graphs},
	abstract     = {Software security risk analysis is an important part of improving software quality. In previous research we proposed countermeasure graphs (CGs), an approach to conduct risk analysis, combining the ideas of different risk analysis approaches. The approach was designed for reuse and easy evolvability to support agile software development. CGs have not been evaluated in industry practice in agile software development. In this research we evaluate the ability of CGs to support practitioners in identifying the most critical threats and countermeasures. The research method used is participatory action research where CGs were evaluated in a series of risk analyses on four different telecom products. With Peltier (used prior to the use of CGs at the company) the practitioners identified attacks with low to medium risk level. CGs allowed practitioners to identify more serious risks (in the first iteration 1 serious threat, 5 high risk threats, and 11 medium threats). The need for tool support was identified very early, tool support allowed the practitioners to play through scenarios of which countermeasures to implement, and supported reuse. The results indicate that CGs support practitioners in identifying high risk security threats, work well in an agile software development context, and are cost-effective.}
}
@inproceedings{10.1007/978-3-642-13792-1_15,
	title        = {Prioritizing Countermeasures through the Countermeasure Method for Software Security (CM-Sec)},
	author       = {Baca, Dejan and Petersen, Kai},
	year         = 2010,
	booktitle    = {Proceedings of the 11th International Conference on Product-Focused Software Process Improvement},
	location     = {Limerick, Ireland},
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	series       = {PROFES'10},
	pages        = {176–190},
	doi          = {10.1007/978-3-642-13792-1_15},
	isbn         = 3642137911,
	url          = {https://doi.org/10.1007/978-3-642-13792-1_15},
	abstract     = {Software security is an important quality aspect of a software system. Therefore, it is important to integrate software security touch points throughout the development life-cycle. So far, the focus of touch points in the early phases has been on the identification of threats and attacks. In this paper we propose a novel method focusing on the end product by prioritizing countermeasures. The method provides an extension to attack trees and a process for identification and prioritization of countermeasures. The approach has been applied on an open-source application and showed that countermeasures could be identified. Furthermore, an analysis of the effectiveness and cost-efficiency of the countermeasures could be provided.},
	numpages     = 15
}
@article{article,
	title        = {Improving software security with static automated code analysis in an industry setting},
	author       = {Baca, Dejan and Carlsson, Bengt and Petersen, Kai and Lundberg, Lars},
	year         = 2013,
	month        = {03},
	journal      = {Softw., Pract. Exper.},
	volume       = 43,
	pages        = {259--279},
	doi          = {10.1002/spe.2109},
	abstract     = {Software security can be improved by identifying and correcting vulnerabilities. In order to reduce the cost of rework, vulnerabilities should be detected as early and efficiently as possible. Static automated code analysis is an approach for early detection. So far, only few empirical studies have been conducted in an industrial context to evaluate static automated code analysis. A case study was conducted to evaluate static code analysis in industry focusing on defect detection capability, deployment, and usage of static automated code analysis with a focus on software security. We identified that the tool was capable of detecting memory related vulnerabilities, but few vulnerabilities of other types. The deployment of the tool played an important role in its success as an early vulnerability detector, but also the developers perception of the tools merit. Classifying the warnings from the tool was harder for the developers than to correct them. The correction of false positives in some cases created new vulnerabilities in previously safe code. With regard to defect detection ability, we conclude that static code analysis is able to identify vulnerabilities in different categories. In terms of deployment, we conclude that the tool should be integrated with bug reporting systems, and developers need to share the responsibility for classifying and reporting warnings. With regard to tool usage by developers, we propose to use multiple persons (at least two) in classifying a warning. The same goes for making the decision of how to act based on the warning.},
	keywords     = {}
}
@article{Dede1989TheEO,
	title        = {The Evolution of Information Technology: Implications for Curriculum},
	author       = {Chris Dede},
	year         = 1989,
	journal      = {Educational Leadership},
	volume       = 7,
	pages        = {23--26},
	abstract     = {Technological Evolution Since World War II, the performance capabilities of computers and telecommunications have been doubling every few years at constant cost. For example, a decade ago $3,500 could buy a new Apple II microcomputer. Today, $6,800 — the same amount of purchasing power (adjusted for 10 years of inflation) — can buy a new Macintosh II microcomputer. The Macintosh handles 4 times the information at 16 times the speed, preprogrammed and reprogrammable memory are both about 20 times larger, disk storage is about 90 times larger, and the display has 7 times the resolution and 16 times the number of colors. Comparable figures could be cited for other brands of machines.},
	keywords     = {}
}
@article{article,
	title        = {Delay and Secrecy: Does Industry Sponsorship Jeopardize Disclosure of Academic Research?},
	author       = {Czarnitzki, Dirk and Grimpe, Christoph and Toole, Andrew},
	year         = 2011,
	month        = {01},
	journal      = {Industrial and Corporate Change},
	volume       = 24,
	pages        = {},
	doi          = {10.2139/ssrn.1759433},
	abstract     = {The viability of modern open science norms and practices depend on public disclosure of new knowledge, methods, and materials. Aggregate data from the OECD show a broad shift in the institutional financing structure that supports academic research from public to private sponsorship. This article examines the relationship between industry sponsorship and restrictions on disclosure using individual-level data on German academic researchers. Accounting for self-selection into extramural sponsorship, our evidence strongly supports the perspective that industry sponsorship jeopardizes public disclosure of academic research.},
	keywords     = {}
}
@inproceedings{10.1145/2593752.2593757,
	title        = {Alignment Practices Affect Distances in Software Development: A Theory and a Model},
	author       = {Bjarnason, Elizabeth and Smolander, Kari and Engstr\"{o}m, Emelie and Runeson, Per},
	year         = 2014,
	booktitle    = {Proceedings of the 3rd SEMAT Workshop on General Theories of Software Engineering},
	location     = {Hyderabad, India},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {GTSE 2014},
	pages        = {21–31},
	doi          = {10.1145/2593752.2593757},
	isbn         = 9781450328500,
	url          = {https://doi.org/10.1145/2593752.2593757},
	abstract     = {Coordinating a software project across distances is challenging. Even without geographical and time zone distances, other distances within a project can cause communication gaps. For example, organisational and cognitive distances between product owners and development-near roles such as developers and testers can lead to weak alignment of the software and the business requirements. Applying good software development practices, known to enhance alignment, can alleviate these challenges. We present a theoretical model called the Gap Model of how alignment practices affect different types of distances. This model has been inductively generated from empirical data. We also present an initial version of a theory based on this model that explains, at a general level, how practices affect communication within a project by impacting distances between people, activities and artefacts. The presented results provide a basis for further research and can be used by software organisations to improve on software practice.},
	numpages     = 11,
	keywords     = {software development, empirical software engineering, distances}
}
@article{article,
	title        = {Challenges and practices in aligning requirements with verification and validation: A case study of six companies},
	author       = {Bjarnason, Elizabeth and Runeson, Per and Borg, Markus and Unterkalmsteiner, Michael and EngstrÃ¶m, Emelie and Regnell, BjÃ¶rn and Sabaliauskaite, Giedre},
	year         = 2013,
	month        = {07},
	journal      = {Empirical Software Engineering},
	volume       = 19,
	pages        = {},
	doi          = {10.1007/s10664-013-9263-y},
	abstract     = {Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two.We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VVactivities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.},
	keywords     = {}
}
@article{ENGSTROM201014,
	title        = {A systematic review on regression test selection techniques},
	author       = {Emelie EngstrÃ¶m and Per Runeson and Mats Skoglund},
	year         = 2010,
	journal      = {Information and Software Technology},
	volume       = 52,
	number       = 1,
	pages        = {14--30},
	doi          = {https://doi.org/10.1016/j.infsof.2009.07.001},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584909001219},
	keywords     = {Regression testing, Test selection, Systematic review, Empirical studies},
	abstract     = {Regression testing is verifying that previously functioning software remains after a change. With the goal of finding a basis for further research in a joint industry-academia research project, we conducted a systematic review of empirical evaluations of regression test selection techniques. We identified 27 papers reporting 36 empirical studies, 21 experiments and 15 case studies. In total 28 techniques for regression test selection are evaluated. We present a qualitative analysis of the findings, an overview of techniques for regression test selection and related empirical evidence. No technique was found clearly superior since the results depend on many varying factors. We identified a need for empirical studies where concepts are evaluated rather than small variations in technical implementations.}
}
@inproceedings{6823890,
	title        = {Supporting Regression Test Scoping with Visual Analytics},
	author       = {Engström, Emelie and Mantylä, Mika and Runeson, Per and Borg, Markus},
	year         = 2014,
	month        = {March},
	booktitle    = {2014 IEEE Seventh International Conference on Software Testing, Verification and Validation},
	volume       = {},
	number       = {},
	pages        = {283--292},
	doi          = {10.1109/ICST.2014.41},
	issn         = {2159-4848},
	abstract     = {Background: Test managers have to repeatedly select test cases for test activities during evolution of large software systems. Researchers have widely studied automated test scoping, but have not fully investigated decision support with human interaction. We previously proposed the introduction of visual analytics for this purpose. Aim: In this empirical study we investigate how to design such decision support. Method: We explored the use of visual analytics using heat maps of historical test data for test scoping support by letting test managers evaluate prototype visualizations in three focus groups with in total nine industrial test experts. Results: All test managers in the study found the visual analytics useful for supporting test planning. However, our results show that different tasks and contexts require different types of visualizations. Conclusion: Important properties for test planning support are: ability to overview testing from different perspectives, ability to filter and zoom to compare subsets of the testing with respect to various attributes and the ability to manipulate the subset under analysis by selecting and deselecting test cases. Our results may be used to support the introduction of visual test analytics in practice.},
	keywords     = {Regression test, Decision support, Visual analytics}
}
@article{FERNANDEZMEDINA2005463,
	title        = {Designing secure databases},
	author       = {Eduardo FernÃ¡ndez-Medina and Mario Piattini},
	year         = 2005,
	journal      = {Information and Software Technology},
	volume       = 47,
	number       = 7,
	pages        = {463--477},
	doi          = {https://doi.org/10.1016/j.infsof.2004.09.013},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584904001429},
	keywords     = {Secure databases, Database design, Unified Modeling Language, Object Constraint Language},
	abstract     = {Security is an important issue that must be considered as a fundamental requirement in information systems development, and particularly in database design. Therefore security, as a further quality property of software, must be tackled at all stages of the development. The most extended secure database model is the multilevel model, which permits the classification of information according to its confidentiality, and considers mandatory access control. Nevertheless, the problem is that no database design methodologies that consider security (and therefore secure database models) across the entire life cycle, particularly at the earliest stages currently exist. Therefore it is not possible to design secure databases appropriately. Our aim is to solve this problem by proposing a methodology for the design of secure databases. In addition to this methodology, we have defined some models that allow us to include security information in the database model, and a constraint language to define security constraints. As a result, we can specify a fine-grained classification of the information, defining with a high degree of accuracy which properties each user has to own in order to be able to access each piece of information. The methodology consists of four stages: requirements gathering; database analysis; multilevel relational logical design; and specific logical design. The first three stages define activities to analyze and design a secure database, thus producing a general secure database model. The last stage is made up of activities that adapt the general secure data model to one of the most popular secure database management systems: Oracle9i Label Security. This methodology has been used in a genuine case by the Data Processing Center of Provincial Government. In order to support the methodology, we have implemented an extension of Rational Rose, including and managing security information and constraints in the first stages of the methodology.}
}
@article{MORAVALENTIN200417,
	title        = {Determining factors in the success of R&D cooperative agreements between firms and research organizations},
	author       = {Eva M Mora-Valentin and Angeles Montoro-Sanchez and Luis A Guerras-Martin},
	year         = 2004,
	journal      = {Research Policy},
	volume       = 33,
	number       = 1,
	pages        = {17--40},
	doi          = {https://doi.org/10.1016/S0048-7333(03)00087-8},
	issn         = {0048-7333},
	url          = {https://www.sciencedirect.com/science/article/pii/S0048733303000878},
	keywords     = {R&D cooperation, Cooperation between firms and research organizations, Success in cooperative agreements, Organizational and contextual factors},
	abstract     = {The purpose of this paper is to analyze the impact of a series of contextual and organizational factors on the success of 800 cooperative agreements between Spanish firms and research organizations, run between 1995 and 2000. Findings show that the most outstanding factors are, in the case of firms, commitment, previous links, definition of objectives and conflict, whereas for research organizations previous links, communication, commitment, trust and the partnersâ reputation are more relevant. These study not only provides a comprehensive theoretical model to analyze the success of these agreements but is useful both for improving management of cooperation and for fostering collaboration both at a national an international level.}
}
@inproceedings{6571626,
	title        = {Model-Based Test Suite Generation for Function Block Diagrams Using the UPPAAL Model Checker},
	author       = {Enoiu, Eduard Paul and Sundmark, Daniel and Pettersson, Paul},
	year         = 2013,
	month        = {March},
	booktitle    = {2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops},
	volume       = {},
	number       = {},
	pages        = {158--167},
	doi          = {10.1109/ICSTW.2013.27},
	issn         = {},
	abstract     = {A method for model-based test generation of safety-critical embedded applications using Programmable Logic Controllers and implemented in a programming language such as Function Block Diagram (FBD) is described. The FBD component model is based on the IEC 1131 standard and it is used primarily for embedded systems, in which timeliness is an important property to be tested. Our method involves the transformation of FBD programs with timed annotations into timed automata models which are used to automatically generate test suites. Specifically we demonstrate how to use model transformation for formalization and model-checking of FBD programs using the UPPAAL tool. Many benefits emerge from this method, including the ability to automatically generate test suites from a formal model in order to ensure compliance to strict quality requirements including unit testing and specific coverage measurements. The approach is experimentally assessed on a train control system in terms of consumed resources.},
	keywords     = {}
}
@inproceedings{6605711,
	title        = {MOS: An integrated model-based and search-based testing tool for Function Block Diagrams},
	author       = {Enoiu, Eduard Paul and Doganay, Kivanc and Bohlin, Markus and Sundmark, Daniel and Pettersson, Paul},
	year         = 2013,
	month        = {May},
	booktitle    = {2013 1st International Workshop on Combining Modelling and Search-Based Software Engineering (CMSBSE)},
	volume       = {},
	number       = {},
	pages        = {55--60},
	doi          = {10.1109/CMSBSE.2013.6605711},
	issn         = {},
	abstract     = {In this paper we present a new testing tool for safety critical applications described in Function Block Diagram (FBD) language aimed to support both a model and a search-based approach. Many benefits emerge from this tool, including the ability to automatically generate test suites from an FBD program in order to comply to quality requirements such as component testing and specific coverage measurements. Search-based testing methods are used to generate test data based on executable code rather than the FBD program, alleviating any problems that may arise from the ambiguities that occur while creating FBD programs. Test cases generated by both approaches are executed and used as a way of cross validation. In the current work, we describe the architecture of the tool, its workflow process, and a case study in which the tool has been applied in a real industrial setting to test a train control management system.},
	keywords     = {},
	keywords     = {model-based software testing, search-based software testing, timed automata, programmable logic controllers}
}
@article{ELBERZHAGER20121092,
	title        = {Reducing test effort: A systematic mapping study on existing approaches},
	author       = {Frank Elberzhager and Alla Rosbach and JÃŒrgen MÃŒnch and Robert Eschbach},
	year         = 2012,
	journal      = {Information and Software Technology},
	volume       = 54,
	number       = 10,
	pages        = {1092--1106},
	doi          = {https://doi.org/10.1016/j.infsof.2012.04.007},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584912000894},
	keywords     = {Efficiency improvement, Mapping study, Quality assurance, Software testing, Test effort reduction},
	abstract     = {Context Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50% of the overall development effort. Consequently, one major goal is often to reduce testing effort. Objective The main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments. Method Two researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles. Results In total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches. Conclusion The results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected.}
}
@article{blum1955action,
	title        = {Action research—a scientific approach?},
	author       = {Blum, Fred H},
	year         = 1955,
	journal      = {Philosophy of science},
	publisher    = {Cambridge University Press},
	volume       = 22,
	number       = 1,
	pages        = {1--7},
	abstract     = {The concept of action-research has been developed during the last decade, mainly at the Research Center for Group Dynamics, University of Michigan, Ann Arbor and at the Commission for Community Interrelations of the American Jewish Congress—centers founded by the late Kurt Lewin whose original and creative mind has made many contributions to social-psychological and sociological research. I owe my acquaintance with this new approach to the Research Center, particularly to Ronald Lippitt and Alvin Zander. Yet most of the following observations are based on my research experience during the last four years. The responsibility for what I am saying is, therefore, completely my own.},
	keywords     = {}
}
@article{Susman1978AnAO,
	title        = {An Assessment of the Scientific Merits of Action Research.},
	author       = {Gerald I. Susman and Roger D. Evered},
	year         = 1978,
	journal      = {Administrative Science Quarterly},
	volume       = 23,
	pages        = {582--603},
	abstract     = {December 1978, volume 23 This article describes the deficiencies of positivist science for generating knowledge for use in solving problems that members of organizations face. Action research is introduced as a method for correcting these deficiencies. When action research is tested against the criteria of positivist science, action research is found not to meet its critical tests. The appropriateness of positivist science is questioned as a basis for judging the scientific merits of action research. Action research can base its legitimacy as science in philosophical traditions that are different from those which legitimate positivist science. Criteria and methods of science appropriate to action research are offered.}
}
@article{841782,
	title        = {The push to make software engineering respectable},
	author       = {Pour, G. and Griss, M.L. and Lutz, M.},
	year         = 2000,
	month        = {May},
	journal      = {Computer},
	volume       = 33,
	number       = 5,
	pages        = {35--43},
	doi          = {10.1109/2.841782},
	issn         = {1558-0814},
	abstract     = {A recognized engineering profession must have an established body of knowledge and skill that its practitioners understand and use consistently. After 30 years, there is still a wide gap between the best and the typical software engineering practices. To close this gap, we need a deeper partnership among industry, academia, and professional societies. We have spent some time considering the reasons for SE's immaturity. All of us are heavily involved in both industry and academia and have been active in professional societies that aim to promote SE as a profession. Promotion efforts are by no means limited to the US, but because our experience is primarily with US activities, that is our focus in this article. Our main goal is to explore, from a multifaceted perspective, why we are where we are now and how we can move forward.},
	keywords     = {}
}
@article{GASPAR1998136,
	title        = {Information Technology and the Future of Cities},
	author       = {Jess Gaspar and Edward L. Glaeser},
	year         = 1998,
	journal      = {Journal of Urban Economics},
	volume       = 43,
	number       = 1,
	pages        = {136--156},
	doi          = {https://doi.org/10.1006/juec.1996.2031},
	issn         = {0094-1190},
	url          = {https://www.sciencedirect.com/science/article/pii/S0094119096920318},
	abstract     = {Will improvements in information technology eliminate face-to-face interactions and make cities obsolete? In this paper, we present a model where people make contacts and choose a mode of interaction: meeting face-to-face or communicating electronically. Cities are a means of reducing the fixed travel costs involved in face-to-face interactions. When telecommunications technology improves, there will be two opposing effects on cities and face-to-face interactions. First, some relationships that would have been face-to-face will be conducted electronically. Second, the increase in frequency of contact between individuals caused by improvements in telecommunications technology may result in more face-to-face interactions. If the second effect dominates, telecommunications improvements will complement both face-to-face interactions and cities. Our empirical work suggests that telecommunications may be a complement to, or at least not a strong substitute for, cities and face-to-face interactions.}
}
@article{article,
	title        = {The Anatomy of a Design Theory},
	author       = {Gregor, Shirley and Shirley, and Jones, and David,},
	year         = 2007,
	month        = {05},
	journal      = {Journal of the Association for Information Systems},
	volume       = 8,
	pages        = {312-},
	doi          = {10.17705/1jais.00129},
	abstract     = {Design work and design knowledge in Information Systems (IS) is important for both research and practice. Yet there has been comparatively little critical attention paid to the problem of specifying design theory so that it can be communicated, justified, and developed cumulatively. In this essay we focus on the structural components or anatomy of design theories in IS as a special class of theory. In doing so, we aim to extend the work of Walls, Widemeyer and El Sawy (1992) on the specification of information systems design theories (ISDT), drawing on other streams of thought on design research and theory to provide a basis for a more systematic and useable formulation of these theories. We identify eight separate components of design theories: (1) purpose and scope, (2) constructs, (3) principles of form and function, (4) artifact mutability, (5) testable propositions, (6) justificatory knowledge (kernel theories), (7) principles of implementation, and (8) an expository instantiation. This specification includes components missing in the Walls et al. adaptation of Dubin (1978) and Simon (1969) and also addresses explicitly problems associated with the role of instantiations and the specification of design theories for methodologies and interventions as well as for products and applications. The essay is significant as the unambiguous establishment of design knowledge as theory gives a sounder base for arguments for the rigor and legitimacy of IS as an applied discipline and for its continuing progress. A craft can proceed with the copying of one example of a design artifact by one artisan after another. A discipline cannot.},
	keywords     = {}
}
@article{MUNIR2014375,
	title        = {Considering rigor and relevance when evaluating test driven development: A systematic review},
	author       = {Hussan Munir and Misagh Moayyed and Kai Petersen},
	year         = 2014,
	journal      = {Information and Software Technology},
	volume       = 56,
	number       = 4,
	pages        = {375--394},
	doi          = {https://doi.org/10.1016/j.infsof.2014.01.002},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584914000135},
	keywords     = {Test-driven development (TDD), Test-last development (TLD), Internal code quality, External code quality, Productivity},
	abstract     = {Context Test driven development (TDD) has been extensively researched and compared to traditional approaches (test last development, TLD). Existing literature reviews show varying results for TDD. Objective This study investigates how the conclusions of existing literature reviews change when taking two study quality dimension into account, namely rigor and relevance. Method In this study a systematic literature review has been conducted and the results of the identified primary studies have been analyzed with respect to rigor and relevance scores using the assessment rubric proposed by Ivarsson and Gorschek 2011. Rigor and relevance are rated on a scale, which is explained in this paper. Four categories of studies were defined based on high/low rigor and relevance. Results We found that studies in the four categories come to different conclusions. In particular, studies with a high rigor and relevance scores show clear results for improvement in external quality, which seem to come with a loss of productivity. At the same time high rigor and relevance studies only investigate a small set of variables. Other categories contain many studies showing no difference, hence biasing the results negatively for the overall set of primary studies. Given the classification differences to previous literature reviews could be highlighted. Conclusion Strong indications are obtained that external quality is positively influenced, which has to be further substantiated by industry experiments and longitudinal case studies. Future studies in the high rigor and relevance category would contribute largely by focusing on a wider set of outcome variables (e.g. internal code quality). We also conclude that considering rigor and relevance in TDD evaluation is important given the differences in results between categories and in comparison to previous reviews.}
}
@article{hevner2004design,
	title        = {Design science in information systems research},
	author       = {Hevner, Alan R and March, Salvatore T and Park, Jinsoo and Ram, Sudha},
	year         = 2004,
	journal      = {MIS quarterly},
	publisher    = {JSTOR},
	pages        = {75--105},
	abstract     = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
	keywords     = {}
}
@article{6671600,
	title        = {Heuristics for Thread-Level Speculation in Web Applications},
	author       = {Martinsen, Jan Kasper and Grahn, Håkan and Isberg, Anders},
	year         = 2014,
	month        = {July},
	journal      = {IEEE Computer Architecture Letters},
	volume       = 13,
	number       = 2,
	pages        = {77--80},
	doi          = {10.1109/L-CA.2013.26},
	issn         = {1556-6064},
	abstract     = {JavaScript is a sequential programming language, and Thread-Level Speculation has been proposed to dynamically extract parallelism in order to take advantage of parallel hardware. In previous work, we have showed significant speed-ups with a simple on/off speculation heuristic. In this paper, we propose and evaluate three heuristics for dynamically adapt the speculation: a 2-bit heuristic, an exponential heuristic, and a combination of these two. Our results show that the combined heuristic is able to both increase the number of successful speculations and decrease the execution time for 15 popular web applications.},
	keywords     = {C.1.4 Parallel Architectures, C.1.4.f Speculative multi-threading}
}
@article{McKay2001TheDI,
	title        = {The dual imperatives of action research},
	author       = {Judy McKay and Peter Marshall},
	year         = 2001,
	journal      = {Inf. Technol. People},
	volume       = 14,
	pages        = {46--59},
	abstract     = {Action research (AR) is not without its critics, and those who reject some of the paradigmatic assumptions embodied in AR maintain that AR is little more than consultancy, that it is impossible to establish causal relationships, that it is difficult to generalize from AR studies, that there is a risk of researcher bias, and that generally speaking, it lacks some of the key qualities that are normally associated with rigorous research. The authors are sensitive to such criticisms, for although they are committed action researchers, they have elsewhere voiced their concerns about the quality of AR practice in the field of information systems. The authors argue that part of the issue concerns the way in which we currently conceptualize AR. In this article, the argument for a deeper and more reflective analysis of the meaning and full implications of AR is developed, culminating in a model of AR being developed that explicitly includes both a problem solving interest cycle and a research interest cycle. Important implications of this new model are articulated, with examples to illustrate these points being drawn from a real‐life AR study.}
}
@inproceedings{inproceedings,
	title        = {Challenges in Flexible Safety-Critical Software Development - An Industrial Qualitative Survey},
	author       = {Notander, Jesper and HÃ¶st, Martin and Runeson, Per},
	year         = 2013,
	month        = {06},
	volume       = 7983,
	pages        = {283--297},
	doi          = {10.1007/978-3-642-39259-7_23},
	isbn         = {978-3-642-39258-0},
	abstract     = {Context. Development of safety-critical systems is mostly governed by process-heavy paradigms, while increasing demands on flexibility and agility also reach this domain. Objectives. We wanted to explore in more detail the industrial needs and challenges when facing this trend. Method. We launched a qualitative survey, interviewing engineers from four companies in four different industry domains. Results. The survey identifies human factors (skills, experience, and attitudes) being key in safety-critical systems development, as well as good documentation. Certification cost is related to change frequency, which is limiting flexibility. Component reuse and iterative processes were found to increase adaptability to changing customer needs. Conclusions. We conclude that agile development and flexibility may co-exist with safety-critical software development, although there are specific challenges to address.}
}
@inproceedings{Calof2014AnOO,
	title        = {An Overview of the Demise of Nortel Networks and Key Lessons Learned : Systemic effects in environment , resilience and black-cloud formation},
	author       = {Jonathan Calof and Gregory Richards and Laurent Mirabeau and Hussein T. Mouftah and Peter Mackinnon and Peter Chapman},
	year         = 2014,
	abstract     = {Nortel’s customers were clear: they did not want to switch suppliers. However, these same customers felt that Nortel had given them no choice but to switch. By 2006 customers said that Nortel was no longer the same company that they had come to trust and rely on, and they doubted that the company would survive beyond the next five or six years. As a result, they were unwilling to commit to Nortel for any longer-term related procurement. In June 2009, Nortel Networks, once Canada’s largest and most successful technology company, announced that it would sell all its business units and effectively end over 100 years of operations. While other companies in this industry, including Ericsson, Nokia, Siemens, Alcatel, Lucent and Cisco, had run into difficulties because of the massive changes sweeping across the competitive environment, each has found a way to survive to this day}
}
@article{636668,
	title        = {Collaborations: closing the industry-academia gap},
	author       = {Beckman, K. and Coulter, N. and Khajenoori, S. and Mead, N.R.},
	year         = 1997,
	month        = {Nov},
	journal      = {IEEE Software},
	volume       = 14,
	number       = 6,
	pages        = {49--57},
	doi          = {10.1109/52.636668},
	issn         = {1937-4194},
	abstract     = {When it comes to software engineering education, there is a gap between what industry needs and what universities offer. To close this gap, the authors propose a comprehensive collaboration between academic software engineering programs and industry. They offer a model for this collaboration and highlight three real-world ventures.},
	keywords     = {}
}
@inproceedings{870456,
	title        = {Applying and adjusting a software process improvement model in practice: the use of the IDEAL model in a small software enterprise},
	author       = {Kautz, K. and Hansen, H.W. and Thaysen, K.},
	year         = 2000,
	month        = {June},
	booktitle    = {Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium},
	volume       = {},
	number       = {},
	pages        = {626--633},
	doi          = {10.1145/337180.337492},
	issn         = {0270-5257},
	abstract     = {Software process improvement is a demanding and complex undertaking. To support the constitution and implementation of software process improvement schemes the Software Engineering Institute (SEI) proposes a framework, the so-called IDEAL model. This model is based on experiences from large organizations. The aim of the research described was to investigate the suitability of the model for small software enterprises. It has therefore been deployed and adjusted for successful use in a small Danish software company. The course of the project and the application of the model are presented and the case is reflected on the background of current knowledge about managing software process improvement as organizational change.},
	keywords     = {}
}
@inproceedings{5316010,
	title        = {Context in industrial software engineering research},
	author       = {Petersen, Kai and Wohlin, Claes},
	year         = 2009,
	month        = {Oct},
	booktitle    = {2009 3rd International Symposium on Empirical Software Engineering and Measurement},
	volume       = {},
	number       = {},
	pages        = {401--404},
	doi          = {10.1109/ESEM.2009.5316010},
	issn         = {1949-3789},
	abstract     = {In order to draw valid conclusions when aggregating evidence it is important to describe the context in which industrial studies were conducted. This paper structures the context for empirical industrial studies and provides a checklist. The aim is to aid researchers in making informed decisions concerning which parts of the context to include in the descriptions. Furthermore, descriptions of industrial studies were surveyed.},
	keywords     = {}
}
@inproceedings{6693226,
	title        = {Worldviews, Research Methods, and their Relationship to Validity in Empirical Software Engineering Research},
	author       = {Petersen, Kai and Gencel, Cigdem},
	year         = 2013,
	month        = {Oct},
	booktitle    = {2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement},
	volume       = {},
	number       = {},
	pages        = {81--89},
	doi          = {10.1109/IWSM-Mensura.2013.22},
	issn         = {},
	abstract     = {Background - Validity threats should be considered and consistently reported to judge the value of an empirical software engineering research study. The relevance of specific threats for a particular research study depends on the worldview or philosophical worldview of the researchers of the study. Problem/Gap - In software engineering, different categorizations exist, which leads to inconsistent reporting and consideration of threats. Contribution - In this paper, we relate different worldviews to software engineering research methods, identify generic categories for validity threats, and provide a categorization of validity threats with respect to their relevance for different world views. Thereafter, we provide a checklist aiding researchers in identifying relevant threats. Method - Different threat categorizations and threats have been identified in literature, and are reflected on in relation to software engineering research. Results - Software engineering is dominated by the pragmatist worldviews, and therefore use multiple methods in research. Maxwell's categorization of validity threats has been chosen as very suitable for reporting validity threats in software engineering research. Conclusion - We recommend to follow a checklist approach, and reporting first the philosophical worldview of the researcher when doing the research, the research methods and all threats relevant, including open, reduced, and mitigated threats.},
	keywords     = {}
}
@article{article,
	title        = {An elicitation instrument for operationalising GQM+Strategies (GQM+S-EI)},
	author       = {Petersen, Kai and Gencel, Cigdem and Asghari, Negin and Betz, Stefanie},
	year         = 2014,
	month        = {08},
	journal      = {Empirical Software Engineering},
	pages        = {},
	doi          = {10.1007/S.10664-014-9306-z},
	abstract     = {Context: A recent approach for measurement program planning, GQM+Strategies, provides an important extension to existing approaches linking measurements and improvement activities to strategic goals and ways to achieve these goals. There is a need for instruments aiding in eliciting information from stakeholders to use GQM+Strategies. The success of GQM+Strategies highly depends on accurately identifying goals, strategies and information needs from stakeholders. Objective: The research aims at providing an instrument (called GQM+SEI), aiding practitioners to accurately elicit information needed by GQM+Strategies (capturing goals, strategies and information needs). Method: The research included two phases. In the first phase, using action research method, the GQM+S-EI was designed in three iterations in Ericsson AB. Thereafter, a case study was conducted to evaluate whether the information elicited with the designed instrument following the defined process was accurate and complete. Results: We identified that the industry requires elicitation instruments that are capable to elicit information from stakeholders, not having to know about the concepts (e.g. goals and strategies). The case study results showed that our proposed instrument is capable of accurately and completely capturing the needed information from the stakeholders. Conclusions: We conclude that GQM+S-EI can be used for accurately and completely eliciting the information needed by goal driven measurement frameworks. The instrument has been successfully transferred to Ericsson AB for measurement program planning.},
	keywords     = {}
}
@inproceedings{6328160,
	title        = {Adapting the Lean Enterprise Self-Assessment Tool for the Software Development Domain},
	author       = {Karvonen, Teemu and Rodriguez, Pilar and Kuvaja, Pasi and Mikkonen, Kirsi and Oivo, Markku},
	year         = 2012,
	month        = {Sep.},
	booktitle    = {2012 38th Euromicro Conference on Software Engineering and Advanced Applications},
	volume       = {},
	number       = {},
	pages        = {266--273},
	doi          = {10.1109/SEAA.2012.51},
	issn         = {2376-9505},
	abstract     = {Lean principles have attracted the attention of software development companies due to their potential to improve competitiveness. However, the application of such principles in the software domain is still in its infancy. This paper presents a proposal for adapting the Lean Enterprise Self-Assessment Tool (LESAT) to guide the transformation of software development companies toward Lean. LESAT, developed by the Lean Advancement Initiative (LAI) at the Massachusetts Institute of Technology (MIT), has been widely used in other domains. In this study, concepts and expressions of LESAT were analyzed and mapped to software development following the ISO/IEC 12207 standard. Seven assessment items concerning life-cycle processes were modified from the original LESAT. The modified LESAT for software was compared with a lean assessment approach called "Lean amplifier, " which has been developed and successfully used in practice by Ericsson R&amp;D in Finland. The results indicated that LESAT may complement lean assessment in the software domain at enterprise level, involving the entire value stream. Moreover, they clearly emphasized the role of leadership in the transformation.},
	keywords     = {lean, lean software development, lean transformation, assessment, LESAT, enterprise, software engineering}
}
@article{article,
	title        = {On theory development in design science research: anatomy of a research project},
	author       = {Kuechler, William and Vaishnavi, Vijay},
	year         = 2008,
	month        = {01},
	journal      = {EJIS},
	volume       = 17,
	pages        = {489--504},
	abstract     = {The common understanding of design science research in information systems (DSRIS) continues to evolve. Only in the broadest terms has there been consensus: that DSRIS involves, in some way, learning through the act of building. However, what is to be built – the definition of the DSRIS artifact – and how it is to be built – the methodology of DSRIS – has drawn increasing discussion in recent years. The relationship of DSRIS to theory continues to make up a significant part of the discussion: how theory should inform DSRIS and whether or not DSRIS can or should be instrumental in developing and refining theory. In this paper, we present the exegesis of a DSRIS research project in which creating a (prescriptive) design theory through the process of developing and testing an information systems artifact is inextricably bound to the testing and refinement of its kernel theory.},
	keywords     = {}
}
