@book{10.5555/1121729,
	title        = {A Guide To The Project Management Body Of Knowledge (PMBOK Guides)},
	year         = 2004,
	publisher    = {Project Management Institute},
	isbn         = {193069945X},
	abstract     = {Its hard to imagine a time when A Guide to the Project Management Body of Knowledge (PMBOK® Guide) wasnt around. Yet, just twenty years ago, PMI volunteers first sat down to distill the project management body of knowledge. Their hard work eventually became the PMBOK® Guide, now considered one of the most essential tools in the profession and is the de facto global standard for the industry. With more than a million copies of the PMBOK® Guide2000 Edition in use, PMI has received numerous positive comments and suggestions for improvements. Methodical updates occur on a four-year cycle to ensure PMIs commitment to continually improve and revise the information contained in this essential reference manual. Users will find a number of changes when they upgrade from the PMBOK® Guide2000 Edition. One of the most important changes is the criteria for included information, which evolved from generally accepted on most projects, most of the time to generally recognized as good practice on most projects, most of the time. Unique to the PMBOK® GuideThird Edition is the increased clarity and emphasis on processes, including highlighting that the five Process Groups are the key to the management of projects. As the updated official standard of the worlds leading project management organization, PMBOK® Guide Third Edition is an essential reference tool for every project management practitioners library.}
}
@inproceedings{10.1145/1297846.1297941,
	title        = {Continuing Professional Development by Practitioner Integrated Learning},
	author       = {Borjesson, Anna and Pareto, Lars and Snis, Ulrika Lundh and Staron, Miroslaw},
	year         = 2007,
	booktitle    = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
	location     = {Montreal, Quebec, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {OOPSLA '07},
	pages        = {897–907},
	doi          = {10.1145/1297846.1297941},
	isbn         = 9781595938657,
	url          = {https://doi.org/10.1145/1297846.1297941},
	abstract     = {To prevent skilled professionals from being phased out or forced into professions for which they are not talented, organized forms of lifelong learning are needed. Continuing professional development is an approach supporting lifelong learning. This approach is however criticized for being expensive and not providing the necessary knowledge. In response to this, we have executed a study in order to understand how universities can effectively support continuous professional development. By involving industry professionals as participants in university courses using problem based learning, we have designed what we call Practitioner Integrated Learning (PIL). This learning approach has shown positive effects in terms of level of learning, realism, knowledge diffusion, study load and costs. We present a 15-months action research project integrating 16 industry managers and 16 university students in a continuing professional development effort. Based on this study, we argue that PIL is a learning approach that effectively supports continuing professional development.},
	numpages     = 11,
	keywords     = {continuing professional development, PBL, lifelong learning, practitioner integrated learning, action research, problem based learning}
}
@inproceedings{1191351,
	title        = {On a partnership between software industry and academia},
	author       = {Kornecki, A.J. and Khajenoori, S. and Gluch, D. and Kameli, N.},
	year         = 2003,
	month        = {March},
	booktitle    = {Proceedings 16th Conference on Software Engineering Education and Training, 2003. (CSEE&T 2003).},
	volume       = {},
	number       = {},
	pages        = {60--69},
	doi          = {10.1109/CSEE.2003.1191351},
	issn         = {1093-0175},
	abstract     = {This paper discusses a role for industry in software engineering education, specifically presenting a university-industry partnership between the Cardiac Rhythm Management (CRM) organization at the Guidant Corporation and Embry-Riddle Aeronautical University (ERAU). The focus of the partnership is technology transition. The partnership involves fostering students' professional development, providing students experience solving realworld problems, and exploring modern directions of software engineering. The critical component of the partnership is a student-oriented research laboratory. After discussing the background and history of the project, we focus on the partnership's accomplishments. These include facilitating the transition of graduates from student to employee by developing in them extended software engineering skills and in-depth understanding of the application domain.},
	keywords     = {}
}
@inproceedings{6928785,
	title        = {Network Analysis of a Large Scale Open Source Project},
	author       = {Orucevic-Alagic, Alma and Höst, Martin},
	year         = 2014,
	month        = {Aug},
	booktitle    = {2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
	volume       = {},
	number       = {},
	pages        = {25--29},
	doi          = {10.1109/SEAA.2014.50},
	issn         = {2376-9505},
	abstract     = {One way to understand the structure of an open source community is by applying network analysis to its source code repositories. In this paper a new method for the analysis of committers' networks is proposed. The method deals with directed and weighted committers' networks. The method is then applied to the Android open source project. The analysis results show how a large, company sponsored, and industry backed open source project, i.e. An open source project with the majority of the community members affiliated with the industry, is structured. In particular, it shows that the involvement of an entire industry eco system within a company sponsored open source project does not imply more equal distribution of the participating community members' influences in terms of committers' networks.},
	keywords     = {}
}
@article{article,
	title        = {Design Science in Information Systems Research},
	author       = {Hevner, Alan and R, Alan and March, Salvatore and T, Salvatore and Park, and Park, Jinsoo and Ram, and Sudha,},
	year         = 2004,
	month        = {03},
	journal      = {Management Information Systems Quarterly},
	volume       = 28,
	pages        = {75-},
	abstract     = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
	keywords     = {nformation Systems research meth-odologies, design science, design artifact, busi-ness environment, technology infrastructure,search strategies, experimental methods, creativity}
}
@inproceedings{10.1145/2494091.2494110,
	title        = {Some like It Hot: Automating an Electric Kettle Using PalCom},
	author       = {Magnusson, Boris and Johnsson, Bj\"{o}rn A.},
	year         = 2013,
	booktitle    = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
	location     = {Zurich, Switzerland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {UbiComp '13 Adjunct},
	pages        = {63–66},
	doi          = {10.1145/2494091.2494110},
	isbn         = 9781450322157,
	url          = {https://doi.org/10.1145/2494091.2494110},
	abstract     = {In this demo we will show how devices from different vendors, using different protocols, can be combined and made to work together without detailed low-level programming by the user. The small example we have chosen uses a radio-controlled power socket from one vendor and a temperature sensor from another vendor. We use these to create a remotely controlled electric kettle, which keeps the water at the point of boiling, ready to make tea at any time. We also show how we very easy can use a mobile phone for remote control and monitoring of the kettle. It is all built with a simple-to-use graphical user interface offered by the PalCom middleware, and will be modified as part of the demo.},
	numpages     = 4,
	keywords     = {palcom, ad-hoc composition, services, middleware, pervasive systems, devices}
}
@inproceedings{inproceedings,
	title        = {Factors Influencing Industrial Practices of Software Architecture Evaluation: An Empirical Investigation},
	author       = {Ali Babar, Muhammad and Bass, Len and Gorton, Ian},
	year         = 2007,
	month        = {07},
	volume       = 4880,
	pages        = {90--107},
	doi          = {10.1007/978-3-540-77619-2_6},
	abstract     = {To support software architecture evaluation practices, several efforts have been made to provide a basis for comparing and assessing evaluation methods, document various best practices, and report the factors that may influence industrial practices. However, there has been no study to explore the experiences and perceptions of architects for determining the factors that influence architecture evaluation practices in a wide range of organizations. Hence, there is little empirically founded knowledge available on the factors that influence the industrial practices of software architecture evaluation. The goal of this paper is to report the results of an empirical study aimed at gaining an understanding of different factors involved in evaluating architectures in industry. The results of this study shed light on the factors that influence architecture evaluation practices based on the experiences and perception of architects who regularly evaluate architectures of various sizes of applications. It also discusses some of the strategies that practitioners apply to deal with the influence of the identified factors.},
	keywords     = {Focus Group, Quality Attribute, Software Architecture, Focus Group Session, Governance Framework}
}
@article{GENCEL20133091,
	title        = {A decision support framework for metrics selection in goal-based measurement programs: GQM-DSFMS},
	author       = {Cigdem Gencel and Kai Petersen and Aftab Ahmad Mughal and Muhammad Imran Iqbal},
	year         = 2013,
	journal      = {Journal of Systems and Software},
	volume       = 86,
	number       = 12,
	pages        = {3091--3108},
	doi          = {https://doi.org/10.1016/j.jss.2013.07.022},
	issn         = {0164-1212},
	url          = {https://www.sciencedirect.com/science/article/pii/S0164121213001726},
	keywords     = {Software measurement program, Goal based measurement, Goal Question Metric, GQM, Decision support, Optimization, Prioritization},
	abstract     = {Software organizations face challenges in managing and sustaining their measurement programs over time. The complexity of measurement programs increase with exploding number of goals and metrics to collect. At the same time, organizations usually have limited budget and resources for metrics collection. It has been recognized for quite a while that there is the need for prioritizing goals, which then ought to drive the selection of metrics. On the other hand, the dynamic nature of the organizations requires measurement programs to adapt to the changes in the stakeholders, their goals, information needs and priorities. Therefore, it is crucial for organizations to use structured approaches that provide transparency, traceability and guidance in choosing an optimum set of metrics that would address the highest priority information needs considering limited resources. This paper proposes a decision support framework for metrics selection (DSFMS) which is built upon the widely used Goal Question Metric (GQM) approach. The core of the framework includes an iterative goal-based metrics selection process incorporating decision making mechanisms in metrics selection, a pre-defined Attributes/Metrics Repository, and a Traceability Model among GQM elements. We also discuss alternative prioritization and optimization techniques for organizations to tailor the framework according to their needs. The evaluation of the GQM-DSFMS framework was done through a case study in a CMMI Level 3 software company.}
}
@article{BACA20132411,
	title        = {Countermeasure graphs for software security risk assessment: An action research},
	author       = {Dejan Baca and Kai Petersen},
	year         = 2013,
	journal      = {Journal of Systems and Software},
	volume       = 86,
	number       = 9,
	pages        = {2411--2428},
	doi          = {https://doi.org/10.1016/j.jss.2013.04.023},
	issn         = {0164-1212},
	url          = {https://www.sciencedirect.com/science/article/pii/S0164121213001027},
	keywords     = {Software security, Risk analysis, Countermeasure graphs},
	abstract     = {Software security risk analysis is an important part of improving software quality. In previous research we proposed countermeasure graphs (CGs), an approach to conduct risk analysis, combining the ideas of different risk analysis approaches. The approach was designed for reuse and easy evolvability to support agile software development. CGs have not been evaluated in industry practice in agile software development. In this research we evaluate the ability of CGs to support practitioners in identifying the most critical threats and countermeasures. The research method used is participatory action research where CGs were evaluated in a series of risk analyses on four different telecom products. With Peltier (used prior to the use of CGs at the company) the practitioners identified attacks with low to medium risk level. CGs allowed practitioners to identify more serious risks (in the first iteration 1 serious threat, 5 high risk threats, and 11 medium threats). The need for tool support was identified very early, tool support allowed the practitioners to play through scenarios of which countermeasures to implement, and supported reuse. The results indicate that CGs support practitioners in identifying high risk security threats, work well in an agile software development context, and are cost-effective.}
}
@inproceedings{10.1007/978-3-642-13792-1_15,
	title        = {Prioritizing Countermeasures through the Countermeasure Method for Software Security (CM-Sec)},
	author       = {Baca, Dejan and Petersen, Kai},
	year         = 2010,
	booktitle    = {Proceedings of the 11th International Conference on Product-Focused Software Process Improvement},
	location     = {Limerick, Ireland},
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	series       = {PROFES'10},
	pages        = {176–190},
	doi          = {10.1007/978-3-642-13792-1_15},
	isbn         = 3642137911,
	url          = {https://doi.org/10.1007/978-3-642-13792-1_15},
	abstract     = {Software security is an important quality aspect of a software system. Therefore, it is important to integrate software security touch points throughout the development life-cycle. So far, the focus of touch points in the early phases has been on the identification of threats and attacks. In this paper we propose a novel method focusing on the end product by prioritizing countermeasures. The method provides an extension to attack trees and a process for identification and prioritization of countermeasures. The approach has been applied on an open-source application and showed that countermeasures could be identified. Furthermore, an analysis of the effectiveness and cost-efficiency of the countermeasures could be provided.},
	numpages     = 15
}
@article{article,
	title        = {Improving software security with static automated code analysis in an industry setting},
	author       = {Baca, Dejan and Carlsson, Bengt and Petersen, Kai and Lundberg, Lars},
	year         = 2013,
	month        = {03},
	journal      = {Softw., Pract. Exper.},
	volume       = 43,
	pages        = {259--279},
	doi          = {10.1002/spe.2109},
	abstract     = {Software security can be improved by identifying and correcting vulnerabilities. In order to reduce the cost of rework, vulnerabilities should be detected as early and efficiently as possible. Static automated code analysis is an approach for early detection. So far, only few empirical studies have been conducted in an industrial context to evaluate static automated code analysis. A case study was conducted to evaluate static code analysis in industry focusing on defect detection capability, deployment, and usage of static automated code analysis with a focus on software security. We identified that the tool was capable of detecting memory related vulnerabilities, but few vulnerabilities of other types. The deployment of the tool played an important role in its success as an early vulnerability detector, but also the developers perception of the tools merit. Classifying the warnings from the tool was harder for the developers than to correct them. The correction of false positives in some cases created new vulnerabilities in previously safe code. With regard to defect detection ability, we conclude that static code analysis is able to identify vulnerabilities in different categories. In terms of deployment, we conclude that the tool should be integrated with bug reporting systems, and developers need to share the responsibility for classifying and reporting warnings. With regard to tool usage by developers, we propose to use multiple persons (at least two) in classifying a warning. The same goes for making the decision of how to act based on the warning.},
	keywords     = {}
}
@article{Dede1989TheEO,
	title        = {The Evolution of Information Technology: Implications for Curriculum},
	author       = {Chris Dede},
	year         = 1989,
	journal      = {Educational Leadership},
	volume       = 7,
	pages        = {23--26},
	abstract     = {Technological Evolution Since World War II, the performance capabilities of computers and telecommunications have been doubling every few years at constant cost. For example, a decade ago $3,500 could buy a new Apple II microcomputer. Today, $6,800 — the same amount of purchasing power (adjusted for 10 years of inflation) — can buy a new Macintosh II microcomputer. The Macintosh handles 4 times the information at 16 times the speed, preprogrammed and reprogrammable memory are both about 20 times larger, disk storage is about 90 times larger, and the display has 7 times the resolution and 16 times the number of colors. Comparable figures could be cited for other brands of machines.},
	keywords     = {}
}
@article{article,
	title        = {Delay and Secrecy: Does Industry Sponsorship Jeopardize Disclosure of Academic Research?},
	author       = {Czarnitzki, Dirk and Grimpe, Christoph and Toole, Andrew},
	year         = 2011,
	month        = {01},
	journal      = {Industrial and Corporate Change},
	volume       = 24,
	pages        = {},
	doi          = {10.2139/ssrn.1759433},
	abstract     = {The viability of modern open science norms and practices depend on public disclosure of new knowledge, methods, and materials. Aggregate data from the OECD show a broad shift in the institutional financing structure that supports academic research from public to private sponsorship. This article examines the relationship between industry sponsorship and restrictions on disclosure using individual-level data on German academic researchers. Accounting for self-selection into extramural sponsorship, our evidence strongly supports the perspective that industry sponsorship jeopardizes public disclosure of academic research.},
	keywords     = {}
}
@inproceedings{10.1145/2593752.2593757,
	title        = {Alignment Practices Affect Distances in Software Development: A Theory and a Model},
	author       = {Bjarnason, Elizabeth and Smolander, Kari and Engstr\"{o}m, Emelie and Runeson, Per},
	year         = 2014,
	booktitle    = {Proceedings of the 3rd SEMAT Workshop on General Theories of Software Engineering},
	location     = {Hyderabad, India},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {GTSE 2014},
	pages        = {21–31},
	doi          = {10.1145/2593752.2593757},
	isbn         = 9781450328500,
	url          = {https://doi.org/10.1145/2593752.2593757},
	abstract     = {Coordinating a software project across distances is challenging. Even without geographical and time zone distances, other distances within a project can cause communication gaps. For example, organisational and cognitive distances between product owners and development-near roles such as developers and testers can lead to weak alignment of the software and the business requirements. Applying good software development practices, known to enhance alignment, can alleviate these challenges. We present a theoretical model called the Gap Model of how alignment practices affect different types of distances. This model has been inductively generated from empirical data. We also present an initial version of a theory based on this model that explains, at a general level, how practices affect communication within a project by impacting distances between people, activities and artefacts. The presented results provide a basis for further research and can be used by software organisations to improve on software practice.},
	numpages     = 11,
	keywords     = {software development, empirical software engineering, distances}
}
@article{article,
	title        = {Challenges and practices in aligning requirements with verification and validation: A case study of six companies},
	author       = {Bjarnason, Elizabeth and Runeson, Per and Borg, Markus and Unterkalmsteiner, Michael and EngstrÃ¶m, Emelie and Regnell, BjÃ¶rn and Sabaliauskaite, Giedre},
	year         = 2013,
	month        = {07},
	journal      = {Empirical Software Engineering},
	volume       = 19,
	pages        = {},
	doi          = {10.1007/s10664-013-9263-y},
	abstract     = {Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two.We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VVactivities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.},
	keywords     = {}
}
@article{ENGSTROM201014,
	title        = {A systematic review on regression test selection techniques},
	author       = {Emelie EngstrÃ¶m and Per Runeson and Mats Skoglund},
	year         = 2010,
	journal      = {Information and Software Technology},
	volume       = 52,
	number       = 1,
	pages        = {14--30},
	doi          = {https://doi.org/10.1016/j.infsof.2009.07.001},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584909001219},
	keywords     = {Regression testing, Test selection, Systematic review, Empirical studies},
	abstract     = {Regression testing is verifying that previously functioning software remains after a change. With the goal of finding a basis for further research in a joint industry-academia research project, we conducted a systematic review of empirical evaluations of regression test selection techniques. We identified 27 papers reporting 36 empirical studies, 21 experiments and 15 case studies. In total 28 techniques for regression test selection are evaluated. We present a qualitative analysis of the findings, an overview of techniques for regression test selection and related empirical evidence. No technique was found clearly superior since the results depend on many varying factors. We identified a need for empirical studies where concepts are evaluated rather than small variations in technical implementations.}
}
@inproceedings{6823890,
	title        = {Supporting Regression Test Scoping with Visual Analytics},
	author       = {Engström, Emelie and Mantylä, Mika and Runeson, Per and Borg, Markus},
	year         = 2014,
	month        = {March},
	booktitle    = {2014 IEEE Seventh International Conference on Software Testing, Verification and Validation},
	volume       = {},
	number       = {},
	pages        = {283--292},
	doi          = {10.1109/ICST.2014.41},
	issn         = {2159-4848},
	abstract     = {Background: Test managers have to repeatedly select test cases for test activities during evolution of large software systems. Researchers have widely studied automated test scoping, but have not fully investigated decision support with human interaction. We previously proposed the introduction of visual analytics for this purpose. Aim: In this empirical study we investigate how to design such decision support. Method: We explored the use of visual analytics using heat maps of historical test data for test scoping support by letting test managers evaluate prototype visualizations in three focus groups with in total nine industrial test experts. Results: All test managers in the study found the visual analytics useful for supporting test planning. However, our results show that different tasks and contexts require different types of visualizations. Conclusion: Important properties for test planning support are: ability to overview testing from different perspectives, ability to filter and zoom to compare subsets of the testing with respect to various attributes and the ability to manipulate the subset under analysis by selecting and deselecting test cases. Our results may be used to support the introduction of visual test analytics in practice.},
	keywords     = {Regression test, Decision support, Visual analytics}
}
@article{FERNANDEZMEDINA2005463,
	title        = {Designing secure databases},
	author       = {Eduardo FernÃ¡ndez-Medina and Mario Piattini},
	year         = 2005,
	journal      = {Information and Software Technology},
	volume       = 47,
	number       = 7,
	pages        = {463--477},
	doi          = {https://doi.org/10.1016/j.infsof.2004.09.013},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584904001429},
	keywords     = {Secure databases, Database design, Unified Modeling Language, Object Constraint Language},
	abstract     = {Security is an important issue that must be considered as a fundamental requirement in information systems development, and particularly in database design. Therefore security, as a further quality property of software, must be tackled at all stages of the development. The most extended secure database model is the multilevel model, which permits the classification of information according to its confidentiality, and considers mandatory access control. Nevertheless, the problem is that no database design methodologies that consider security (and therefore secure database models) across the entire life cycle, particularly at the earliest stages currently exist. Therefore it is not possible to design secure databases appropriately. Our aim is to solve this problem by proposing a methodology for the design of secure databases. In addition to this methodology, we have defined some models that allow us to include security information in the database model, and a constraint language to define security constraints. As a result, we can specify a fine-grained classification of the information, defining with a high degree of accuracy which properties each user has to own in order to be able to access each piece of information. The methodology consists of four stages: requirements gathering; database analysis; multilevel relational logical design; and specific logical design. The first three stages define activities to analyze and design a secure database, thus producing a general secure database model. The last stage is made up of activities that adapt the general secure data model to one of the most popular secure database management systems: Oracle9i Label Security. This methodology has been used in a genuine case by the Data Processing Center of Provincial Government. In order to support the methodology, we have implemented an extension of Rational Rose, including and managing security information and constraints in the first stages of the methodology.}
}
@article{MORAVALENTIN200417,
	title        = {Determining factors in the success of R&D cooperative agreements between firms and research organizations},
	author       = {Eva M Mora-Valentin and Angeles Montoro-Sanchez and Luis A Guerras-Martin},
	year         = 2004,
	journal      = {Research Policy},
	volume       = 33,
	number       = 1,
	pages        = {17--40},
	doi          = {https://doi.org/10.1016/S0048-7333(03)00087-8},
	issn         = {0048-7333},
	url          = {https://www.sciencedirect.com/science/article/pii/S0048733303000878},
	keywords     = {R&D cooperation, Cooperation between firms and research organizations, Success in cooperative agreements, Organizational and contextual factors},
	abstract     = {The purpose of this paper is to analyze the impact of a series of contextual and organizational factors on the success of 800 cooperative agreements between Spanish firms and research organizations, run between 1995 and 2000. Findings show that the most outstanding factors are, in the case of firms, commitment, previous links, definition of objectives and conflict, whereas for research organizations previous links, communication, commitment, trust and the partnersâ reputation are more relevant. These study not only provides a comprehensive theoretical model to analyze the success of these agreements but is useful both for improving management of cooperation and for fostering collaboration both at a national an international level.}
}
@inproceedings{6571626,
	title        = {Model-Based Test Suite Generation for Function Block Diagrams Using the UPPAAL Model Checker},
	author       = {Enoiu, Eduard Paul and Sundmark, Daniel and Pettersson, Paul},
	year         = 2013,
	month        = {March},
	booktitle    = {2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops},
	volume       = {},
	number       = {},
	pages        = {158--167},
	doi          = {10.1109/ICSTW.2013.27},
	issn         = {},
	abstract     = {A method for model-based test generation of safety-critical embedded applications using Programmable Logic Controllers and implemented in a programming language such as Function Block Diagram (FBD) is described. The FBD component model is based on the IEC 1131 standard and it is used primarily for embedded systems, in which timeliness is an important property to be tested. Our method involves the transformation of FBD programs with timed annotations into timed automata models which are used to automatically generate test suites. Specifically we demonstrate how to use model transformation for formalization and model-checking of FBD programs using the UPPAAL tool. Many benefits emerge from this method, including the ability to automatically generate test suites from a formal model in order to ensure compliance to strict quality requirements including unit testing and specific coverage measurements. The approach is experimentally assessed on a train control system in terms of consumed resources.},
	keywords     = {}
}
@inproceedings{6605711,
	title        = {MOS: An integrated model-based and search-based testing tool for Function Block Diagrams},
	author       = {Enoiu, Eduard Paul and Doganay, Kivanc and Bohlin, Markus and Sundmark, Daniel and Pettersson, Paul},
	year         = 2013,
	month        = {May},
	booktitle    = {2013 1st International Workshop on Combining Modelling and Search-Based Software Engineering (CMSBSE)},
	volume       = {},
	number       = {},
	pages        = {55--60},
	doi          = {10.1109/CMSBSE.2013.6605711},
	issn         = {},
	abstract     = {In this paper we present a new testing tool for safety critical applications described in Function Block Diagram (FBD) language aimed to support both a model and a search-based approach. Many benefits emerge from this tool, including the ability to automatically generate test suites from an FBD program in order to comply to quality requirements such as component testing and specific coverage measurements. Search-based testing methods are used to generate test data based on executable code rather than the FBD program, alleviating any problems that may arise from the ambiguities that occur while creating FBD programs. Test cases generated by both approaches are executed and used as a way of cross validation. In the current work, we describe the architecture of the tool, its workflow process, and a case study in which the tool has been applied in a real industrial setting to test a train control management system.},
	keywords     = {},
	keywords     = {model-based software testing, search-based software testing, timed automata, programmable logic controllers}
}
@article{ELBERZHAGER20121092,
	title        = {Reducing test effort: A systematic mapping study on existing approaches},
	author       = {Frank Elberzhager and Alla Rosbach and JÃŒrgen MÃŒnch and Robert Eschbach},
	year         = 2012,
	journal      = {Information and Software Technology},
	volume       = 54,
	number       = 10,
	pages        = {1092--1106},
	doi          = {https://doi.org/10.1016/j.infsof.2012.04.007},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584912000894},
	keywords     = {Efficiency improvement, Mapping study, Quality assurance, Software testing, Test effort reduction},
	abstract     = {Context Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50% of the overall development effort. Consequently, one major goal is often to reduce testing effort. Objective The main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments. Method Two researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles. Results In total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches. Conclusion The results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected.}
}
@article{blum1955action,
	title        = {Action research—a scientific approach?},
	author       = {Blum, Fred H},
	year         = 1955,
	journal      = {Philosophy of science},
	publisher    = {Cambridge University Press},
	volume       = 22,
	number       = 1,
	pages        = {1--7},
	abstract     = {The concept of action-research has been developed during the last decade, mainly at the Research Center for Group Dynamics, University of Michigan, Ann Arbor and at the Commission for Community Interrelations of the American Jewish Congress—centers founded by the late Kurt Lewin whose original and creative mind has made many contributions to social-psychological and sociological research. I owe my acquaintance with this new approach to the Research Center, particularly to Ronald Lippitt and Alvin Zander. Yet most of the following observations are based on my research experience during the last four years. The responsibility for what I am saying is, therefore, completely my own.},
	keywords     = {}
}
@article{Susman1978AnAO,
	title        = {An Assessment of the Scientific Merits of Action Research.},
	author       = {Gerald I. Susman and Roger D. Evered},
	year         = 1978,
	journal      = {Administrative Science Quarterly},
	volume       = 23,
	pages        = {582--603},
	abstract     = {December 1978, volume 23 This article describes the deficiencies of positivist science for generating knowledge for use in solving problems that members of organizations face. Action research is introduced as a method for correcting these deficiencies. When action research is tested against the criteria of positivist science, action research is found not to meet its critical tests. The appropriateness of positivist science is questioned as a basis for judging the scientific merits of action research. Action research can base its legitimacy as science in philosophical traditions that are different from those which legitimate positivist science. Criteria and methods of science appropriate to action research are offered.}
}
@article{841782,
	title        = {The push to make software engineering respectable},
	author       = {Pour, G. and Griss, M.L. and Lutz, M.},
	year         = 2000,
	month        = {May},
	journal      = {Computer},
	volume       = 33,
	number       = 5,
	pages        = {35--43},
	doi          = {10.1109/2.841782},
	issn         = {1558-0814},
	abstract     = {A recognized engineering profession must have an established body of knowledge and skill that its practitioners understand and use consistently. After 30 years, there is still a wide gap between the best and the typical software engineering practices. To close this gap, we need a deeper partnership among industry, academia, and professional societies. We have spent some time considering the reasons for SE's immaturity. All of us are heavily involved in both industry and academia and have been active in professional societies that aim to promote SE as a profession. Promotion efforts are by no means limited to the US, but because our experience is primarily with US activities, that is our focus in this article. Our main goal is to explore, from a multifaceted perspective, why we are where we are now and how we can move forward.},
	keywords     = {}
}
@article{GASPAR1998136,
	title        = {Information Technology and the Future of Cities},
	author       = {Jess Gaspar and Edward L. Glaeser},
	year         = 1998,
	journal      = {Journal of Urban Economics},
	volume       = 43,
	number       = 1,
	pages        = {136--156},
	doi          = {https://doi.org/10.1006/juec.1996.2031},
	issn         = {0094-1190},
	url          = {https://www.sciencedirect.com/science/article/pii/S0094119096920318},
	abstract     = {Will improvements in information technology eliminate face-to-face interactions and make cities obsolete? In this paper, we present a model where people make contacts and choose a mode of interaction: meeting face-to-face or communicating electronically. Cities are a means of reducing the fixed travel costs involved in face-to-face interactions. When telecommunications technology improves, there will be two opposing effects on cities and face-to-face interactions. First, some relationships that would have been face-to-face will be conducted electronically. Second, the increase in frequency of contact between individuals caused by improvements in telecommunications technology may result in more face-to-face interactions. If the second effect dominates, telecommunications improvements will complement both face-to-face interactions and cities. Our empirical work suggests that telecommunications may be a complement to, or at least not a strong substitute for, cities and face-to-face interactions.}
}
@article{article,
	title        = {The Anatomy of a Design Theory},
	author       = {Gregor, Shirley and Shirley, and Jones, and David,},
	year         = 2007,
	month        = {05},
	journal      = {Journal of the Association for Information Systems},
	volume       = 8,
	pages        = {312-},
	doi          = {10.17705/1jais.00129},
	abstract     = {Design work and design knowledge in Information Systems (IS) is important for both research and practice. Yet there has been comparatively little critical attention paid to the problem of specifying design theory so that it can be communicated, justified, and developed cumulatively. In this essay we focus on the structural components or anatomy of design theories in IS as a special class of theory. In doing so, we aim to extend the work of Walls, Widemeyer and El Sawy (1992) on the specification of information systems design theories (ISDT), drawing on other streams of thought on design research and theory to provide a basis for a more systematic and useable formulation of these theories. We identify eight separate components of design theories: (1) purpose and scope, (2) constructs, (3) principles of form and function, (4) artifact mutability, (5) testable propositions, (6) justificatory knowledge (kernel theories), (7) principles of implementation, and (8) an expository instantiation. This specification includes components missing in the Walls et al. adaptation of Dubin (1978) and Simon (1969) and also addresses explicitly problems associated with the role of instantiations and the specification of design theories for methodologies and interventions as well as for products and applications. The essay is significant as the unambiguous establishment of design knowledge as theory gives a sounder base for arguments for the rigor and legitimacy of IS as an applied discipline and for its continuing progress. A craft can proceed with the copying of one example of a design artifact by one artisan after another. A discipline cannot.},
	keywords     = {}
}
@article{MUNIR2014375,
	title        = {Considering rigor and relevance when evaluating test driven development: A systematic review},
	author       = {Hussan Munir and Misagh Moayyed and Kai Petersen},
	year         = 2014,
	journal      = {Information and Software Technology},
	volume       = 56,
	number       = 4,
	pages        = {375--394},
	doi          = {https://doi.org/10.1016/j.infsof.2014.01.002},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584914000135},
	keywords     = {Test-driven development (TDD), Test-last development (TLD), Internal code quality, External code quality, Productivity},
	abstract     = {Context Test driven development (TDD) has been extensively researched and compared to traditional approaches (test last development, TLD). Existing literature reviews show varying results for TDD. Objective This study investigates how the conclusions of existing literature reviews change when taking two study quality dimension into account, namely rigor and relevance. Method In this study a systematic literature review has been conducted and the results of the identified primary studies have been analyzed with respect to rigor and relevance scores using the assessment rubric proposed by Ivarsson and Gorschek 2011. Rigor and relevance are rated on a scale, which is explained in this paper. Four categories of studies were defined based on high/low rigor and relevance. Results We found that studies in the four categories come to different conclusions. In particular, studies with a high rigor and relevance scores show clear results for improvement in external quality, which seem to come with a loss of productivity. At the same time high rigor and relevance studies only investigate a small set of variables. Other categories contain many studies showing no difference, hence biasing the results negatively for the overall set of primary studies. Given the classification differences to previous literature reviews could be highlighted. Conclusion Strong indications are obtained that external quality is positively influenced, which has to be further substantiated by industry experiments and longitudinal case studies. Future studies in the high rigor and relevance category would contribute largely by focusing on a wider set of outcome variables (e.g. internal code quality). We also conclude that considering rigor and relevance in TDD evaluation is important given the differences in results between categories and in comparison to previous reviews.}
}
@article{hevner2004design,
	title        = {Design science in information systems research},
	author       = {Hevner, Alan R and March, Salvatore T and Park, Jinsoo and Ram, Sudha},
	year         = 2004,
	journal      = {MIS quarterly},
	publisher    = {JSTOR},
	pages        = {75--105},
	abstract     = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
	keywords     = {}
}
@article{6671600,
	title        = {Heuristics for Thread-Level Speculation in Web Applications},
	author       = {Martinsen, Jan Kasper and Grahn, Håkan and Isberg, Anders},
	year         = 2014,
	month        = {July},
	journal      = {IEEE Computer Architecture Letters},
	volume       = 13,
	number       = 2,
	pages        = {77--80},
	doi          = {10.1109/L-CA.2013.26},
	issn         = {1556-6064},
	abstract     = {JavaScript is a sequential programming language, and Thread-Level Speculation has been proposed to dynamically extract parallelism in order to take advantage of parallel hardware. In previous work, we have showed significant speed-ups with a simple on/off speculation heuristic. In this paper, we propose and evaluate three heuristics for dynamically adapt the speculation: a 2-bit heuristic, an exponential heuristic, and a combination of these two. Our results show that the combined heuristic is able to both increase the number of successful speculations and decrease the execution time for 15 popular web applications.},
	keywords     = {C.1.4 Parallel Architectures, C.1.4.f Speculative multi-threading}
}
@article{McKay2001TheDI,
	title        = {The dual imperatives of action research},
	author       = {Judy McKay and Peter Marshall},
	year         = 2001,
	journal      = {Inf. Technol. People},
	volume       = 14,
	pages        = {46--59},
	abstract     = {Action research (AR) is not without its critics, and those who reject some of the paradigmatic assumptions embodied in AR maintain that AR is little more than consultancy, that it is impossible to establish causal relationships, that it is difficult to generalize from AR studies, that there is a risk of researcher bias, and that generally speaking, it lacks some of the key qualities that are normally associated with rigorous research. The authors are sensitive to such criticisms, for although they are committed action researchers, they have elsewhere voiced their concerns about the quality of AR practice in the field of information systems. The authors argue that part of the issue concerns the way in which we currently conceptualize AR. In this article, the argument for a deeper and more reflective analysis of the meaning and full implications of AR is developed, culminating in a model of AR being developed that explicitly includes both a problem solving interest cycle and a research interest cycle. Important implications of this new model are articulated, with examples to illustrate these points being drawn from a real‐life AR study.}
}
@inproceedings{inproceedings,
	title        = {Challenges in Flexible Safety-Critical Software Development - An Industrial Qualitative Survey},
	author       = {Notander, Jesper and HÃ¶st, Martin and Runeson, Per},
	year         = 2013,
	month        = {06},
	volume       = 7983,
	pages        = {283--297},
	doi          = {10.1007/978-3-642-39259-7_23},
	isbn         = {978-3-642-39258-0},
	abstract     = {Context. Development of safety-critical systems is mostly governed by process-heavy paradigms, while increasing demands on flexibility and agility also reach this domain. Objectives. We wanted to explore in more detail the industrial needs and challenges when facing this trend. Method. We launched a qualitative survey, interviewing engineers from four companies in four different industry domains. Results. The survey identifies human factors (skills, experience, and attitudes) being key in safety-critical systems development, as well as good documentation. Certification cost is related to change frequency, which is limiting flexibility. Component reuse and iterative processes were found to increase adaptability to changing customer needs. Conclusions. We conclude that agile development and flexibility may co-exist with safety-critical software development, although there are specific challenges to address.}
}
@inproceedings{Calof2014AnOO,
	title        = {An Overview of the Demise of Nortel Networks and Key Lessons Learned : Systemic effects in environment , resilience and black-cloud formation},
	author       = {Jonathan Calof and Gregory Richards and Laurent Mirabeau and Hussein T. Mouftah and Peter Mackinnon and Peter Chapman},
	year         = 2014,
	abstract     = {Nortel’s customers were clear: they did not want to switch suppliers. However, these same customers felt that Nortel had given them no choice but to switch. By 2006 customers said that Nortel was no longer the same company that they had come to trust and rely on, and they doubted that the company would survive beyond the next five or six years. As a result, they were unwilling to commit to Nortel for any longer-term related procurement. In June 2009, Nortel Networks, once Canada’s largest and most successful technology company, announced that it would sell all its business units and effectively end over 100 years of operations. While other companies in this industry, including Ericsson, Nokia, Siemens, Alcatel, Lucent and Cisco, had run into difficulties because of the massive changes sweeping across the competitive environment, each has found a way to survive to this day}
}
@article{636668,
	title        = {Collaborations: closing the industry-academia gap},
	author       = {Beckman, K. and Coulter, N. and Khajenoori, S. and Mead, N.R.},
	year         = 1997,
	month        = {Nov},
	journal      = {IEEE Software},
	volume       = 14,
	number       = 6,
	pages        = {49--57},
	doi          = {10.1109/52.636668},
	issn         = {1937-4194},
	abstract     = {When it comes to software engineering education, there is a gap between what industry needs and what universities offer. To close this gap, the authors propose a comprehensive collaboration between academic software engineering programs and industry. They offer a model for this collaboration and highlight three real-world ventures.},
	keywords     = {}
}
@inproceedings{870456,
	title        = {Applying and adjusting a software process improvement model in practice: the use of the IDEAL model in a small software enterprise},
	author       = {Kautz, K. and Hansen, H.W. and Thaysen, K.},
	year         = 2000,
	month        = {June},
	booktitle    = {Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium},
	volume       = {},
	number       = {},
	pages        = {626--633},
	doi          = {10.1145/337180.337492},
	issn         = {0270-5257},
	abstract     = {Software process improvement is a demanding and complex undertaking. To support the constitution and implementation of software process improvement schemes the Software Engineering Institute (SEI) proposes a framework, the so-called IDEAL model. This model is based on experiences from large organizations. The aim of the research described was to investigate the suitability of the model for small software enterprises. It has therefore been deployed and adjusted for successful use in a small Danish software company. The course of the project and the application of the model are presented and the case is reflected on the background of current knowledge about managing software process improvement as organizational change.},
	keywords     = {}
}
@inproceedings{5316010,
	title        = {Context in industrial software engineering research},
	author       = {Petersen, Kai and Wohlin, Claes},
	year         = 2009,
	month        = {Oct},
	booktitle    = {2009 3rd International Symposium on Empirical Software Engineering and Measurement},
	volume       = {},
	number       = {},
	pages        = {401--404},
	doi          = {10.1109/ESEM.2009.5316010},
	issn         = {1949-3789},
	abstract     = {In order to draw valid conclusions when aggregating evidence it is important to describe the context in which industrial studies were conducted. This paper structures the context for empirical industrial studies and provides a checklist. The aim is to aid researchers in making informed decisions concerning which parts of the context to include in the descriptions. Furthermore, descriptions of industrial studies were surveyed.},
	keywords     = {}
}
@inproceedings{6693226,
	title        = {Worldviews, Research Methods, and their Relationship to Validity in Empirical Software Engineering Research},
	author       = {Petersen, Kai and Gencel, Cigdem},
	year         = 2013,
	month        = {Oct},
	booktitle    = {2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement},
	volume       = {},
	number       = {},
	pages        = {81--89},
	doi          = {10.1109/IWSM-Mensura.2013.22},
	issn         = {},
	abstract     = {Background - Validity threats should be considered and consistently reported to judge the value of an empirical software engineering research study. The relevance of specific threats for a particular research study depends on the worldview or philosophical worldview of the researchers of the study. Problem/Gap - In software engineering, different categorizations exist, which leads to inconsistent reporting and consideration of threats. Contribution - In this paper, we relate different worldviews to software engineering research methods, identify generic categories for validity threats, and provide a categorization of validity threats with respect to their relevance for different world views. Thereafter, we provide a checklist aiding researchers in identifying relevant threats. Method - Different threat categorizations and threats have been identified in literature, and are reflected on in relation to software engineering research. Results - Software engineering is dominated by the pragmatist worldviews, and therefore use multiple methods in research. Maxwell's categorization of validity threats has been chosen as very suitable for reporting validity threats in software engineering research. Conclusion - We recommend to follow a checklist approach, and reporting first the philosophical worldview of the researcher when doing the research, the research methods and all threats relevant, including open, reduced, and mitigated threats.},
	keywords     = {}
}
@article{article,
	title        = {An elicitation instrument for operationalising GQM+Strategies (GQM+S-EI)},
	author       = {Petersen, Kai and Gencel, Cigdem and Asghari, Negin and Betz, Stefanie},
	year         = 2014,
	month        = {08},
	journal      = {Empirical Software Engineering},
	pages        = {},
	doi          = {10.1007/S.10664-014-9306-z},
	abstract     = {Context: A recent approach for measurement program planning, GQM+Strategies, provides an important extension to existing approaches linking measurements and improvement activities to strategic goals and ways to achieve these goals. There is a need for instruments aiding in eliciting information from stakeholders to use GQM+Strategies. The success of GQM+Strategies highly depends on accurately identifying goals, strategies and information needs from stakeholders. Objective: The research aims at providing an instrument (called GQM+SEI), aiding practitioners to accurately elicit information needed by GQM+Strategies (capturing goals, strategies and information needs). Method: The research included two phases. In the first phase, using action research method, the GQM+S-EI was designed in three iterations in Ericsson AB. Thereafter, a case study was conducted to evaluate whether the information elicited with the designed instrument following the defined process was accurate and complete. Results: We identified that the industry requires elicitation instruments that are capable to elicit information from stakeholders, not having to know about the concepts (e.g. goals and strategies). The case study results showed that our proposed instrument is capable of accurately and completely capturing the needed information from the stakeholders. Conclusions: We conclude that GQM+S-EI can be used for accurately and completely eliciting the information needed by goal driven measurement frameworks. The instrument has been successfully transferred to Ericsson AB for measurement program planning.},
	keywords     = {}
}
@inproceedings{6328160,
	title        = {Adapting the Lean Enterprise Self-Assessment Tool for the Software Development Domain},
	author       = {Karvonen, Teemu and Rodriguez, Pilar and Kuvaja, Pasi and Mikkonen, Kirsi and Oivo, Markku},
	year         = 2012,
	month        = {Sep.},
	booktitle    = {2012 38th Euromicro Conference on Software Engineering and Advanced Applications},
	volume       = {},
	number       = {},
	pages        = {266--273},
	doi          = {10.1109/SEAA.2012.51},
	issn         = {2376-9505},
	abstract     = {Lean principles have attracted the attention of software development companies due to their potential to improve competitiveness. However, the application of such principles in the software domain is still in its infancy. This paper presents a proposal for adapting the Lean Enterprise Self-Assessment Tool (LESAT) to guide the transformation of software development companies toward Lean. LESAT, developed by the Lean Advancement Initiative (LAI) at the Massachusetts Institute of Technology (MIT), has been widely used in other domains. In this study, concepts and expressions of LESAT were analyzed and mapped to software development following the ISO/IEC 12207 standard. Seven assessment items concerning life-cycle processes were modified from the original LESAT. The modified LESAT for software was compared with a lean assessment approach called "Lean amplifier, " which has been developed and successfully used in practice by Ericsson R&amp;D in Finland. The results indicated that LESAT may complement lean assessment in the software domain at enterprise level, involving the entire value stream. Moreover, they clearly emphasized the role of leadership in the transformation.},
	keywords     = {lean, lean software development, lean transformation, assessment, LESAT, enterprise, software engineering}
}
@article{article,
	title        = {On theory development in design science research: anatomy of a research project},
	author       = {Kuechler, William and Vaishnavi, Vijay},
	year         = 2008,
	month        = {01},
	journal      = {EJIS},
	volume       = 17,
	pages        = {489--504},
	abstract     = {The common understanding of design science research in information systems (DSRIS) continues to evolve. Only in the broadest terms has there been consensus: that DSRIS involves, in some way, learning through the act of building. However, what is to be built – the definition of the DSRIS artifact – and how it is to be built – the methodology of DSRIS – has drawn increasing discussion in recent years. The relationship of DSRIS to theory continues to make up a significant part of the discussion: how theory should inform DSRIS and whether or not DSRIS can or should be instrumental in developing and refining theory. In this paper, we present the exegesis of a DSRIS research project in which creating a (prescriptive) design theory through the process of developing and testing an information systems artifact is inextricably bound to the testing and refinement of its kernel theory.},
	keywords     = {}
}
@inproceedings{5071112,
	title        = {Experimental Software Engineering: A Report on the State of the Art},
	author       = {Votta, Lawrence G. and Porter, Adam},
	year         = 1995,
	month        = {April},
	booktitle    = {1995 17th International Conference on Software Engineering},
	volume       = {},
	number       = {},
	pages        = {277--277},
	doi          = {10.1145/225014.225040},
	issn         = {0270-5257},
	abstract     = {The goal of this session is to make the software engineering community aware of the opportunities that exist to pursue such an experimental approach. In the remainder of the essay, we describe an emerging model for empirical work and the language for discussing it. We then focus on the current state of experimental software engineering, the road blocks barring effective progress, and what developers and researchers can do to remove them.},
	keywords     = {}
}
@inproceedings{10.1007/978-3-642-04425-0_15,
	title        = {Concern Visibility in Base Station Development --- An Empirical Investigation},
	author       = {Pareto, Lars and Eriksson, Peter and Ehnebom, Staffan},
	year         = 2009,
	booktitle    = {Proceedings of the 12th International Conference on Model Driven Engineering Languages and Systems},
	location     = {Denver, CO},
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	series       = {MODELS '09},
	pages        = {196–210},
	doi          = {10.1007/978-3-642-04425-0_15},
	isbn         = 9783642044243,
	url          = {https://doi.org/10.1007/978-3-642-04425-0_15},
	abstract     = {Contemporary model driven development tools only partially support the abstractions occurring in complex embedded systems development. The paper presents an interpretive case study in which the concerns held by 7 engineers in a large product developing organization were compared to the concerns supported by the modeling tool in use. The paper's main finding is an empirically grounded catalogue of concerns, categorized with respect to visibility in models and other artefacts in use. In the studied case, 26% of the concerns were visible in the models, whereas 38% were visible elsewhere and 36% not visible at all. The catalogue has been presented to several stakeholders in the unit studied, with positive feedback: particularly appreciated were the notion of concern visibility as indicator of degree of implementation of model driven development, and that concerns have traceable connections to experiences of the unit's engineers.},
	numpages     = 15,
	keywords     = {concerns, telecommunication systems, software architecture, viewpoints, Model driven development (MDD), base stations, aspect oriented modeling (AOM), embedded systems}
}
@article{lee1999rigor,
	title        = {Rigor and relevance in MIS research: Beyond the approach of positivism alone},
	author       = {Lee, Allen S},
	year         = 1999,
	journal      = {MIS quarterly},
	publisher    = {JSTOR},
	pages        = {29--33},
	abstract     = {Benbasat and Zmud offer a diagnosis of "why one tends today to observe a lack of relevance to practice in IS research" and a prescription of guidelines that "the IS academic community might follow to introduce relevance into their research efforts and articles." I will comment, first, on the ramifications of their self-avowed positivist orientation; second, on their model-inuse of what relevant research is (i.e., the instrumental model); and third, on the need for the IS research community to take a broad approach to the matter of relevance. I will also refer to the respective commentaries offered by Applegate, by Davenport and Markus, and by Lyytinen}
}
@article{article,
	title        = {University-industry collaboration: Grafting the entrepreneurial paradigm onto academic structures},
	author       = {Dooley, Lawrence and Kirk, David},
	year         = 2007,
	month        = {08},
	journal      = {European Journal of Innovation Management},
	volume       = 10,
	pages        = {316--332},
	doi          = {10.1108/14601060710776734},
	abstract     = {Purpose – The paper aims to identify the requisite attributes and organisation to be displayed by a research university in order to engage successfully in collaborative research with industry partners. Design/methodology/approach – The conceptual framework contrasts the traditional public funding model against the requirements of the “triple helix” model of government-university-industry research funding. The framework supports the exploration of a case study of a long-standing and successful joint research partnership, the Dundee-Kinases Consortium, which links a world-class life sciences research centre and a group of global pharmaceutical companies. Research limitations/implications – The case study provides a starting point, and additional case examinations will confirm the role of resource competences and organisational capabilities in facilitating performance by way of knowledge generation and transfer between partners. Findings – The design and leadership of the consortium achieves vital performance outcomes, namely: accelerating the production of new knowledge about cell signalling processes relating to serious diseases; and faster transfer of new knowledge into drug development processes of pharmaceutical companies. The development of key enabling capabilities by the university, allied with routines for academic-industry researcher interface, are essential elements of the partnering design. Originality/value – The paper demonstrates that university-industry partnerships build on government-university funding, that university-industry relationships foster new university capabilities, and moreover, that academic publication is not displaced by the requirements of industry partners.},
	keywords     = {}
}
@article{10.1007/s10664-010-9146-4,
	title        = {A Method for Evaluating Rigor and Industrial Relevance of Technology Evaluations},
	author       = {Ivarsson, Martin and Gorschek, Tony},
	year         = 2011,
	month        = {jun},
	journal      = {Empirical Softw. Engg.},
	publisher    = {Kluwer Academic Publishers},
	address      = {USA},
	volume       = 16,
	number       = 3,
	pages        = {365–395},
	doi          = {10.1007/s10664-010-9146-4},
	issn         = {1382-3256},
	url          = {https://doi.org/10.1007/s10664-010-9146-4},
	issue_date   = {June      2011},
	abstract     = {One of the main goals of an applied research field such as software engineering is the transfer and widespread use of research results in industry. To impact industry, researchers developing technologies in academia need to provide tangible evidence of the advantages of using them. This can be done trough step-wise validation, enabling researchers to gradually test and evaluate technologies to finally try them in real settings with real users and applications. The evidence obtained, together with detailed information on how the validation was conducted, offers rich decision support material for industry practitioners seeking to adopt new technologies and researchers looking for an empirical basis on which to build new or refined technologies. This paper presents model for evaluating the rigor and industrial relevance of technology evaluations in software engineering. The model is applied and validated in a comprehensive systematic literature review of evaluations of requirements engineering technologies published in software engineering journals. The aim is to show the applicability of the model and to characterize how evaluations are carried out and reported to evaluate the state-of-research. The review shows that the model can be applied to characterize evaluations in requirements engineering. The findings from applying the model also show that the majority of technology evaluations in requirements engineering lack both industrial relevance and rigor. In addition, the research field does not show any improvements in terms of industrial relevance over time.},
	numpages     = 31,
	keywords     = {Systematic review, Requirements engineering, Technology evaluation}
}
@article{Ivarsson2009TechnologyTD,
	title        = {Technology transfer decision support in requirements engineering research: a systematic review of REj},
	author       = {Martin Ivarsson and Tony Gorschek},
	year         = 2009,
	journal      = {Requirements Engineering},
	volume       = 14,
	pages        = {155--175},
	abstract     = {One of the main goals of an applied research field such as requirements engineering is the transfer of research results to industrial use. To promote industrial adoption of technologies developed in academia, researchers need to provide tangible evidence of the advantages of using them. This can be done through industry validation, enabling researchers to test and validate technologies in a real setting with real users and applications. The evidence obtained, together with detailed information on how the validation was conducted, offers rich decision support material for industrial practitioners seeking to adopt new technologies. This paper presents a comprehensive systematic literature review of all papers published in the Requirements Engineering journal containing any type of technology evaluation. The aim is to gauge the support for technology transfer, i.e., to what degree industrial practitioners can use the reporting of technology evaluations in the journal as decision support for adopting the technologies in industrial practice. Findings show that very few evaluations offer full technology transfer support, i.e., have a realistic scale, application or subjects. The major improvement potential concerning support for technology transfer is found to be the subjects used in the evaluations. Attaining company support, including support for using practitioners as subjects, is vital for technology transfer and for researchers seeking to validate technologies.},
	keywords     = {}
}
@article{article,
	title        = {Implementing requirements engineering processes throughout organizations: success factors and challenges},
	author       = {Kauppinen, Marjo and Vartiainen, Matti and Kontio, J and Kujala, Sari and Sulonen, Reijo},
	year         = 2004,
	month        = 11,
	journal      = {Information and Software Technology},
	volume       = 46,
	pages        = {937--953},
	doi          = {10.1016/j.insof.2004.04.002},
	abstract     = {This paper aims at identifying critical factors affecting organization-wide implementation of requirements engineering (RE) processes. The paper is based on a broad literature review and three longitudinal case studies that were carried out using an action research method. The results indicate that RE process implementation is a demanding undertaking, and its success greatly depends on such human factors as motivation, commitment and enthusiasm. Therefore, it is essential that the RE process is useful for its individual users. Furthermore, the results indicate that organizations can gain benefits from RE by defining a simple RE process, by focusing on a small set of RE practices, and by supporting the systematic usage of these practices.}
}
@article{article,
	title        = {Using a qualitative research method for building a software maintenance methodology},
	author       = {Polo, Macario and Piattini, Mario and Ruiz, Francisco},
	year         = 2002,
	month        = 11,
	journal      = {Softw., Pract. Exper.},
	volume       = 32,
	pages        = {1239--1260},
	doi          = {10.1002/spe.481},
	abstract     = {This article explains our experience of using Action Research to develop a software maintenance methodology involving two organizations: a group of university researchers and a software services organization. The concept of 'methodology' comprises a wide set of elements whose identification, definition and integration is not a trivial task, due to the magnitude of the project and to the different nature of the organizations. The use of Action Research was a key factor in the progress of the research and has been essential in the adoption of the methodology within the software services organization.}
}
@article{Shaw2002WhatMG,
	title        = {What makes good research in software engineering?},
	author       = {Mary Shaw},
	year         = 2002,
	journal      = {International Journal on Software Tools for Technology Transfer},
	volume       = 4,
	pages        = {1--7},
	abstract     = {Abstract.Physics, biology, and medicine have well-refined public explanations of their research processes. Even in simplified form, these provide guidance about what counts as “good research” both inside and outside the field. Software engineering has not yet explicitly identified and explained either our research processes or the ways we recognize excellent work. Science and engineering research fields can be characterized in terms of the kinds of questions they find worth investigating, the research methods they adopt, and the criteria by which they evaluate their results. I will present such a characterization for software engineering, showing the diversity of research strategies and the way they shift as ideas mature. Understanding these strategies should help software engineers design research plans and report the results clearly; it should also help explain the character of software engineering research to computer science at large and to other scientists.}
}
@article{STARON2008782,
	title        = {Predicting weekly defect inflow in large software projects based on project planning and test status},
	author       = {Miroslaw Staron and Wilhelm Meding},
	year         = 2008,
	journal      = {Information and Software Technology},
	volume       = 50,
	number       = 7,
	pages        = {782--796},
	doi          = {https://doi.org/10.1016/j.infsof.2007.10.001},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584907001085},
	keywords     = {Software metrics, Predictions, Defects},
	abstract     = {Defects discovered during the testing phase in software projects need to be removed before the software is shipped to the customers. The removal of defects can constitute a significant amount of effort in a project and project managers are faced with a decision whether to continue development or shift some resources to cope with defect removal. The goal of this research is to improve the practice of project management by providing a method for predicting the number of defects reported into the defect database in the project. In this paper we present a method for predicting the number of defects reported into the defect database in a large software project on a weekly basis. The method is based on using project progress data, in particular the information about the test progress, to predict defect inflow in the next three coming weeks. The results show that the prediction accuracy of our models is up to 72% (mean magnitude of relative error for predictions of 1 week in advance is 28%) when used in ongoing large software projects. The method is intended to support project managers in more accurate adjusting resources in the project, since they are notified in advance about the potentially large effort needed to correct defects.}
}
@article{STARON2006727,
	title        = {Empirical assessment of using stereotypes to improve comprehension of UML models: A set of experiments},
	author       = {Miroslaw Staron and Ludwik Kuzniarz and Claes Wohlin},
	year         = 2006,
	journal      = {Journal of Systems and Software},
	volume       = 79,
	number       = 5,
	pages        = {727--742},
	doi          = {https://doi.org/10.1016/j.jss.2005.09.014},
	issn         = {0164-1212},
	url          = {https://www.sciencedirect.com/science/article/pii/S0164121205001433},
	note         = {Quality Software},
	keywords     = {Empirical study, Modeling},
	abstract     = {Stereotypes were introduced into the Unified Modeling Language to provide means of customizing this general purpose modeling language for its usage in specific application domains. The primary role of stereotypes is to brand an existing model element with specific semantics, but stereotypes can also be used to provide means of a secondary classification of modeling elements. This paper elaborates on the influence of stereotypes on the comprehension of models. The paper describes a set of controlled experiments performed in academia and industry which were aimed at evaluating the role of stereotypes in improving comprehension of UML models. The results of the experiments show that stereotypes play a significant role in the comprehension of models and the improvement achieved both by students and industry professionals.}
}
@article{STARON2009721,
	title        = {A framework for developing measurement systems and its industrial evaluation},
	author       = {Miroslaw Staron and Wilhelm Meding and Christer Nilsson},
	year         = 2009,
	journal      = {Information and Software Technology},
	volume       = 51,
	number       = 4,
	pages        = {721--737},
	doi          = {https://doi.org/10.1016/j.infsof.2008.10.001},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584908001419},
	keywords     = {Software metrics, ISO/IEC 15939, Measurement systems},
	abstract     = {As in every engineering discipline, metrics play an important role in software development, with the difference that almost all software projects need the customization of metrics used. In other engineering disciplines, the notion of a measurement system (i.e. a tool used to collect, calculate, and report quantitative data) is well known and defined, whereas it is not as widely used in software engineering. In this paper we present a framework for developing custom measurement systems and its industrial evaluation in a software development unit within Ericsson. The results include the framework for designing measurement systems and its evaluation in real life projects at the company. The results show that with the help of ISO/IEC standards, measurement systems can be effectively used in software industry and that the presented framework improves the way of working with metrics. This paper contributes with the presentation of how automation of metrics collection and processing can be successfully introduced into a large organization and shows the benefits of it: increased efficiency of metrics collection, increased adoption of metrics in the organization, independence from individuals and standardized nomenclature for metrics in the organization.}
}
@article{MACCORMACK20121309,
	title        = {Exploring the duality between product and organizational architectures: A test of the âmirroringâ hypothesis},
	author       = {Alan MacCormack and Carliss Baldwin and John Rusnak},
	year         = 2012,
	journal      = {Research Policy},
	volume       = 41,
	number       = 8,
	pages        = {1309--1324},
	doi          = {https://doi.org/10.1016/j.respol.2012.04.011},
	issn         = {0048-7333},
	url          = {https://www.sciencedirect.com/science/article/pii/S0048733312001205},
	keywords     = {Organizational design, Product design, Architecture, Modularity, Open-source software},
	abstract     = {A variety of academic studies argue that a relationship exists between the structure of an organization and the design of the products that this organization produces. Specifically, products tend to âmirrorâ the architectures of the organizations in which they are developed. This dynamic occurs because the organization's governance structures, problem solving routines and communication patterns constrain the space in which it searches for new solutions. Such a relationship is important, given that product architecture has been shown to be an important predictor of product performance, product variety, process flexibility and even the path of industry evolution. We explore this relationship in the software industry. Our research takes advantage of a natural experiment, in that we observe products that fulfill the same function being developed by very different organizational forms. At one extreme are commercial software firms, in which the organizational participants are tightly-coupled, with respect to their goals, structure and behavior. At the other, are open source software communities, in which the participants are much more loosely-coupled by comparison. The mirroring hypothesis predicts that these different organizational forms will produce products with distinctly different architectures. Specifically, loosely-coupled organizations will develop more modular designs than tightly-coupled organizations. We test this hypothesis, using a sample of matched-pair products. We find strong evidence to support the mirroring hypothesis. In all of the pairs we examine, the product developed by the loosely-coupled organization is significantly more modular than the product from the tightly-coupled organization. We measure modularity by capturing the level of coupling between a product's components. The magnitude of the differences is substantialâup to a factor of six, in terms of the potential for a design change in one component to propagate to others. Our results have significant managerial implications, in highlighting the impact of organizational design decisions on the technical structure of the artifacts that these organizations subsequently develop.}
}
@article{MARCH1995251,
	title        = {Design and natural science research on information technology},
	author       = {Salvatore T. March and Gerald F. Smith},
	year         = 1995,
	journal      = {Decision Support Systems},
	volume       = 15,
	number       = 4,
	pages        = {251--266},
	doi          = {https://doi.org/10.1016/0167-9236(94)00041-2},
	issn         = {0167-9236},
	url          = {https://www.sciencedirect.com/science/article/pii/0167923694000412},
	keywords     = {Information system research, Design science, Natural science, Information technology},
	abstract     = {Research in IT must address the design tasks faced by practitioners. Real problems must be properly conceptualized and represented, appropriate techniques for their solution must be constructed, and solutions must be implemented and evaluated using appropriate criteria. If significant progress is to be made, IT research must also develop an understanding of how and why IT systems work or do not work. Such an understanding must tie together natural laws governing IT systems with natural laws governing the environments in which they operate. This paper presents a two dimensional framework for research in information technology. The first dimension is based on broad types of design and natural science research activities: build, evaluate, theorize, and justify. The second dimension is based on broad types of outputs produced by design research: representational constructs, models, methods, and instantiations. We argue that both design science and natural science activities are needed to insure that IT research is both relevant and effective.}
}
@article{PERKMANN20081884,
	title        = {Engaging the scholar: Three types of academic consulting and their impact on universities and industry},
	author       = {Markus Perkmann and Kathryn Walsh},
	year         = 2008,
	journal      = {Research Policy},
	volume       = 37,
	number       = 10,
	pages        = {1884--1891},
	doi          = {https://doi.org/10.1016/j.respol.2008.07.009},
	issn         = {0048-7333},
	url          = {https://www.sciencedirect.com/science/article/pii/S0048733308001571},
	note         = {Special Section Knowledge Dynamics out of Balance: Knowledge Biased, Skewed and Unmatched},
	keywords     = {Academic consulting, Faculty consulting, Universityâindustry relations, Commercialization, Science technology interface},
	abstract     = {We present a conceptual framework of academic consulting and explore its impacts on universities and the benefits to innovating firms. We distinguish between three types of academic consulting: opportunity-driven, commercialization-driven and research-driven. Exploring the implications of these different types, first, we postulate that consulting has limited impact on biasing academic research towards more âappliedâ themes. Secondly, while we expect research-driven consulting activities to be positively associated with research productivity, opportunity-driven consulting will have a negative impact. Thirdly, we differentiate between different functions of academic consulting for different types of firms.}
}
@article{Perkmann2007UniversityIR,
	title        = {University Industry Relationships and Open Innovation: Towards a Research Agenda},
	author       = {Markus Perkmann and Kathryn Walsh},
	year         = 2007,
	journal      = {IO: Productivity},
	abstract     = {Organizations increasingly rely on external sources of innovation via inter-organizational network relationships. This paper explores the diffusion and characteristics of collaborative relationships between universities and industry, and develops a research agenda informed by an open innovation perspective. A framework is proposed, distinguishing university industry relationships from other mechanisms such as technology transfer or human mobility. On the basis of the existing body of research, the role of practices such as collaborative research, university industry research centres, contract research and academic consulting is analysed. The evidence suggests that such university industry relationships are widely practised, whereby differences exist across industries and scientific disciplines. While most existing research focuses on the effects of university industry links on innovation-specific variables such as patents or firm innovativeness, the organizational dynamics of these relationships remain under-researched. A detailed research agenda addresses research needs in two main areas: search and match processes between universities and firms, and the organization and management of collaborative relationships.}
}
@inproceedings{inproceedings,
	title        = {REARM: A Reuse-Based Economic Model for Software Reference Architectures},
	author       = {MartÃ­nez-FernÃ¡ndez, Silverio and Ayala, Claudia and Franch, Xavier and Marques, Helena},
	year         = 2013,
	month        = {06},
	pages        = {97--112},
	doi          = {10.1007/978-3-642-38977-1_7},
	isbn         = {978-3-642-38976-4},
	abstract     = {To remain competitive, organizations are challenged to make informed and feasible value-driven design decisions in order to ensure the quality of their software systems. However, there is a lack of support for evaluating the economic impact of these decisions with regard to software reference architectures. This damages the communication among architects and management, which can result in poor decisions. This paper aims at ameliorating this problem by presenting a pragmatic preliminary economic model to perform cost-benefit analysis on the adoption of software reference architectures as a key asset for optimizing architectural decision-making. The model is based on existing value-based metrics and economics-driven models used in other areas. A preliminary validation based on a retrospective study showed the ability of the model to support a cost-benefit analysis presented to the management of an IT consulting company. This validation involved a cost-benefit analysis related to reuse and maintenance; other qualities will be integrated as our research progresses.}
}
@inproceedings{inproceedings,
	title        = {Benefits and Drawbacks of Reference Architectures},
	author       = {MartÃ­nez-FernÃ¡ndez, Silverio and Ayala, Claudia and Franch, Xavier and Marques, Helena},
	year         = 2013,
	month        = {07},
	volume       = 7957,
	pages        = {307--310},
	doi          = {10.1007/978-3-642-39031-9_26},
	isbn         = {978-3-642-39030-2},
	abstract     = {Reference architectures (RA) have been studied to create a consistent notion of what constitutes them as well as their benefits and drawbacks. However, few empirical studies have been conducted to provide evidence that support the claims made. To increase this evidence, this paper investigates the actual industrial practice of using RAs. The study consists of a survey with 28 stakeholders from everis, a multinational consulting company based in Spain. We report the findings and contextualize them with previous research.}
}
@inproceedings{10.1145/2601248.2601282,
	title        = {Artifacts of Software Reference Architectures: A Case Study},
	author       = {Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Ayala, Claudia and Franch, Xavier and Marques, Helena Martins},
	year         = 2014,
	booktitle    = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
	location     = {London, England, United Kingdom},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {EASE '14},
	doi          = {10.1145/2601248.2601282},
	isbn         = 9781450324762,
	url          = {https://doi.org/10.1145/2601248.2601282},
	abstract     = {Context: Software reference architectures (SRA) have emerged as an approach to systematically reuse architectural knowledge and software elements in the development of software systems. Over the last years, research has been conducted to uncover the artifacts that SRAs provide in order to build software systems. However, empirical studies have not focused on providing industrial evidence about such artifacts. Aim: This paper investigates which artifacts constitute an SRA, how SRAs are designed, the potential reuse of SRA's artifacts, and how they are used in practice. Method: The study consists of a case study made in collaboration with a multinational consulting company that designs SRAs for diverse client organizations. A total of nine European client organizations that use an SRA participated in the study. We analyzed available documentation and contacted 28 practitioners. Results: In the nine analyzed projects, we observed that the artifacts that constitute an SRA are mainly software elements, guidelines and documentation. The design and implementation of SRAs are influenced by the reuse of artifacts from previous software system development and experiences, and the reuse of an SRA across different business domains may be possible when they are platform-oriented. Regarding SRAs usage, we observed that conformance checking is seldom performed. Conclusions: This study reports artifacts of SRAs as stated by practitioners in order to help software architects and scientists in the inception, design, and application of SRAs.},
	articleno    = 42,
	numpages     = 10,
	keywords     = {empirical software engineering, case study, software reuse, software reference architecture}
}
@inproceedings{MartnezFernndez2013AFF,
	title        = {A Framework for Software Reference Architecture Analysis and Review},
	author       = {Silverio Mart{\'i}nez-Fern{\'a}ndez and Claudia P. Ayala and Xavier Franch and Helena Martins Marques and David Ameller},
	year         = 2013,
	booktitle    = {CIbSE},
	abstract     = {Tight time-to-market needs pushes software companies and IT con- sulting firms to continuously look for techniques to improve their IT services in general, and the design of software architectures in particular. The use of soft- ware reference architectures allows IT consulting firms reusing architectural knowledge and components in a systematic way. In return, IT consulting firms face the need to analyze the return on investment in software reference architec- tures for organizations, and to review these reference architectures in order to ensure their quality and incremental improvement. Little support exists to help IT consulting firms to face these challenges. In this paper we present an empiri- cal framework aimed to support the analysis and review of software reference architectures and their use in IT projects by harvesting relevant evidence from the wide spectrum of involved stakeholders. Such a framework comes from an action research approach held in everis, an IT consulting firm. We report the issues found so far}
}
@inproceedings{10.1145/1062455.1062555,
	title        = {Developing Use Cases and Scenarios in the Requirements Process},
	author       = {Maiden, Neil and Robertson, Suzanne},
	year         = 2005,
	booktitle    = {Proceedings of the 27th International Conference on Software Engineering},
	location     = {St. Louis, MO, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '05},
	pages        = {561–570},
	doi          = {10.1145/1062455.1062555},
	isbn         = 1581139632,
	url          = {https://doi.org/10.1145/1062455.1062555},
	abstract     = {Scenarios are often used for discovering requirements using established techniques, but how such scenarios are initially developed is not so well understood. This experience paper reports the application of one scenario-based approach - RESCUE - to discover requirements for DMAN, an air traffic management system for the UK's National Air Traffic Services. A retrospective analysis of the DMAN use cases, scenarios and requirements artifacts revealed the importance of diverse information sources in the specification of use cases that enabled systematic requirements discovery. Results were used to explore 3 research questions that arose in previous studies. The paper reports lessons from this experience and offers guidelines that practitioners can apply in their requirements processes and academics can use to inform their research.},
	numpages     = 10,
	keywords     = {scenarios, requirements, use cases}
}
@article{article,
	title        = {The validity of action research - validity in action research},
	author       = {Eikeland, Olav},
	year         = 2006,
	month        = {01},
	pages        = {},
	abstract     = {Given the widespread practice of different forms of action research in a variety of venues throughout the world, it is inevitable that the question of the validity of action research would emerge. How real, how authentic, how truthful is action research? What level of confidence can be placed in the research? What criteria ought to determine the validity—the truthfulness and accuracy, the appropriateness, the logic and the technical adequacy of the action research process or any action research study? In this chapter, I address these questions and propose a reconceptualization of the term; I also describe different approaches to determining the validity of action research studies.}
}
@article{469759,
	title        = {The 4+1 View Model of architecture},
	author       = {Kruchten, P.B.},
	year         = 1995,
	month        = {Nov},
	journal      = {IEEE Software},
	volume       = 12,
	number       = 6,
	pages        = {42--50},
	doi          = {10.1109/52.469759},
	issn         = {1937-4194},
	abstract     = {The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them. The logical view describes the design's object model when an object-oriented design method is used. To design an application that is very data driven, you can use an alternative approach to develop some other form of logical view, such as an entity-relationship diagram. The process view describes the design's concurrency and synchronization aspects. The physical view describes the mapping of the software onto the hardware and reflects its distributed aspect. The development view describes the software's static organization in its development environment.&lt;<ETX>&gt;</ETX>},
	keywords     = {}
}
@inproceedings{10.1145/302405.302684,
	title        = {Haemo Dialysis Software Architecture Design Experiences},
	author       = {Bengtsson, PerOlof and Bosch, Jan},
	year         = 1999,
	booktitle    = {Proceedings of the 21st International Conference on Software Engineering},
	location     = {Los Angeles, California, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '99},
	pages        = {516–525},
	doi          = {10.1145/302405.302684},
	isbn         = 1581130740,
	url          = {https://doi.org/10.1145/302405.302684},
	numpages     = 10,
	keywords     = {software architecture, software architecture design},
	abstract     = {In this paper we present the experiences and architecture from a research project conducted in cooperation with two industry partners. The goal of the project was to reengineer an existing system for haemo dialysis machines into a domain specific software architecture. Our main experiences are (1) architecture design is an iterative and incremental process, (2) software quality requires a context, (3) quality attribute assessment methods are too detailed for use during architectural design, (4) application domain concepts are not the best abstractions, (5) aesthetics guides the architect in finding potential weaknesses in the architecture, (6) it is extremely hard to decide when an architecture design is ready, and (7) documenting software architectures is an important problem. We also present the architecture and design rational to give a basis for our experiences. We evaluated the resulting architecture by implementing a prototype application.}
}
@article{Runeson2008GuidelinesFC,
	title        = {Guidelines for conducting and reporting case study research in software engineering},
	author       = {Per Runeson and Martin H{\"o}st},
	year         = 2008,
	journal      = {Empirical Software Engineering},
	volume       = 14,
	pages        = {131--164},
	abstract     = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.}
}
@inproceedings{5316013,
	title        = {Action research use in software engineering: An initial survey},
	author       = {Santos, Paulo Sergio Medeiros dos and Travassos, Guilherme Horta},
	year         = 2009,
	month        = {Oct},
	booktitle    = {2009 3rd International Symposium on Empirical Software Engineering and Measurement},
	volume       = {},
	number       = {},
	pages        = {414--417},
	doi          = {10.1109/ESEM.2009.5316013},
	issn         = {1949-3789},
	abstract     = {This paper presents a literature survey of action research (AR) studies published in nine major Software Engineering (SE) journals and three conference proceedings in the period 1993 to June 2009. A strict selection based on distinguishing SE from Information Systems research has identified 16 papers. Although they represent a very small fraction of the studies being conducted in SE, such papers concern with different SE contexts allowing to get information about the increasing tendency in the AR use in software engineering. However, as shown by the initial results, SE researchers should invest more on rigor when defining, applying and reporting AR studies inSE.},
	keywords     = {}
}
@article{10.2753/MIS0742-1222240302,
	title        = {A Design Science Research Methodology for Information Systems Research},
	author       = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus and Chatterjee, Samir},
	year         = 2007,
	month        = {dec},
	journal      = {J. Manage. Inf. Syst.},
	publisher    = {M. E. Sharpe, Inc.},
	address      = {USA},
	volume       = 24,
	number       = 3,
	pages        = {45–77},
	doi          = {10.2753/MIS0742-1222240302},
	issn         = {0742-1222},
	url          = {https://doi.org/10.2753/MIS0742-1222240302},
	issue_date   = {Number 3 / Winter 2007-2008},
	abstract     = {The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
	numpages     = 33,
	keywords     = {Design Theory, Methodology, Process Model, Design Science, Design Science Research, Case Study, Mental Model}
}
@article{poulin1997economics,
	title        = {The economics of product line development},
	author       = {Poulin, J},
	year         = 1997,
	journal      = {International Journal of Applied Software Technology},
	publisher    = {IAPC},
	volume       = 3,
	pages        = {15--28},
	abstract     = {This paper uses an established reuse metric and return on investment (ROI) model to present a new metric for estimating the financial benefit of software development within product lines. This metric can help organizations develop a business case to support the early development of shared software for use across families of related applications and products. The discussion and metrics apply (1) early in the software life-cycle for estimating product line benefits, and (2) after product release for calculating the benefits that resulted from product line development.}
}
@article{10.5555/374468.374476,
	title        = {Investigating Information Systems with Action Research},
	author       = {Baskerville, Richard L.},
	year         = 1999,
	month        = {nov},
	journal      = {Commun. AIS},
	publisher    = {Association for Information Systems},
	address      = {USA},
	volume       = 2,
	number       = {3es},
	pages        = {4–es},
	issue_date   = {Nov. 1999},
	numpages     = 32,
	abstract     = {Action research is an established research method in use in the social and medical sciences since the mid-twentieth century, and has increased in importance for information systems toward the end of the 1990s. Its particular philosophic context is couched in strongly post-positivist assumptions such as idiographic and interpretive research ideals. Action research has developed a history within information systems that can be explicitly linked to early work by Lewin and the Tavistock Institute. Action research varies in form, and responds to particular problem domains. The most typical form is a participatory method based on a five-step model, which is exemplified by published IS research.}
}
@article{GLASS2002491,
	title        = {Research in software engineering: an analysis of the literature},
	author       = {R.L. Glass and I. Vessey and V. Ramesh},
	year         = 2002,
	journal      = {Information and Software Technology},
	volume       = 44,
	number       = 8,
	pages        = {491--506},
	doi          = {https://doi.org/10.1016/S0950-5849(02)00049-6},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584902000496},
	keywords     = {Topic: computing research, Research approach: evaluative-other, Research method: literature analysis, Reference discipline: not applicable, Level of analysis: profession},
	abstract     = {In this paper, we examine the state of software engineering (SE) research from the point of view of the following research questions:1.What topics do SE researchers address?2.What research approaches do SE researchers use?3.What research methods do SE researchers use?4.On what reference disciplines does SE research depend?5.At what levels of analysis do SE researchers conduct research? To answer those questions, we examined 369 papers in six leading research journals in the SE field, answering those research questions for each paper. From that examination, we conclude that SE research is diverse regarding topic, narrow regarding research approach and method, inwardly-focused regarding reference discipline, and technically focused (as opposed to behaviorally focused) regarding level of analysis. We pass no judgment on the SE field as a result of these findings. Instead, we present them as groundwork for future SE research efforts.}
}
@article{10.5555/374168.374465,
	title        = {An IS Research Relevancy Manifesto},
	author       = {Westfall, Ralph},
	year         = 1999,
	month        = {sep},
	journal      = {Commun. AIS},
	publisher    = {Association for Information Systems},
	address      = {USA},
	volume       = 2,
	number       = {2es},
	pages        = {5–es},
	issue_date   = {Sept. 1999},
	numpages     = 56,
	abstract     = {Many practitioners believe academic IS research is not relevant. I argue that our research, and the underlying rewards system that drives it, needs to respond to these concerns. We need to be more relevant to meet the increasing needs of our students, the organizations that hire them, and the larger society. To analyze the issues, I develop three different scenarios of where the IS field could be 10 years from now. The following visions of the future identify the implications of different levels of adaptation to relevance-related environmental pressures. Scenario 1: Minimal Adaptation. The IS field is shrinking, largely due to competition from newly established schools of information technology. The traditional paper-based journals continue to dominate. Their slow publication cycles, in contrast to the rapid rate of change in the IT industries, mean that most technical topics and many current managerial issues are excluded from the research that generates the greatest institutional rewards. However a market analysis indicates that we can still do relevant research in categories such as: 1) issues contrary to commercial interests; 2) unsolved problems; 3) issues economically unattractive to commercial researchers; 4) issues where 1 With apologies to James Martin for the similarity to the title of one [Martin, 1984] of his many books}
}
@article{GLASS199463,
	title        = {An assessment of systems and software engineering scholars and institutions},
	author       = {Robert L. Glass},
	year         = 1994,
	journal      = {Journal of Systems and Software},
	volume       = 27,
	number       = 1,
	pages        = {63--67},
	doi          = {https://doi.org/10.1016/0164-1212(94)90115-5},
	issn         = {0164-1212},
	url          = {https://www.sciencedirect.com/science/article/pii/0164121294901155},
	abstract     = {Who are the most published authors in the field of systems and software engineering (SSE)? Which are the most published institutions? It is the intention of this article to answer those questions. It is the expectation of the Journal of Systems and Software to make this an annual event, publishing each year a cumulative summary of the most prolific authors and institutions in the field. The methodology of the study (including the journals surveyed) and its limitations will be discussed later. It is important to note here, however, that this study is clearly about SSE and not (for example) computer science or information systems. Both the study methodology and, as will be noted below, the findings, are deliberately and specifically different from what would have been learned in studies focused on those other disciplines. Here are the findings.}
}
@inproceedings{Rodrguez2013BuildingLT,
	title        = {Building lean thinking in a telecom software development organization: strengths and challenges},
	author       = {Pilar Rodr{\'i}guez and Kirsi S. Mikkonen and Pasi Kuvaja and Markku Oivo and Juan Garbajosa},
	year         = 2013,
	booktitle    = {ICSSP 2013},
	abstract     = {The potential shown by Lean in different domains has aroused interest in the software industry. However, it remains unclear how Lean can be effectively applied in a domain such as software development that is fundamentally different from manufacturing. This study explores how Lean principles are implemented in software development companies and the challenges that arise when applying Lean Software Development. For that, a case study was conducted at Ericsson R&D Finland, which successfully adopted Scrum in 2009 and subsequently started a comprehensible transition to Lean in 2010. Focus groups were conducted with company representatives to help devise a questionnaire supporting the creation of a Lean mindset in the company (Team Amplifier). Afterwards, the questionnaire was used in 16 teams based in Finland, Hungary and China to evaluate the status of the transformation. By using Lean thinking, Ericsson R&D Finland has made important improvements to the quality of its products, customer satisfaction and transparency within the organization. Moreover, build times have been reduced over ten times and the number of commits per day has increased roughly five times.The study makes two main contributions to research. First, the main factors that have enabled Ericsson R&D’’s achievements are analysed. Elements such as ‘network of product owners’, ‘continuous integration’, ‘work in progress limits’ and ‘communities of practice’ have been identified as being of fundamental importance. Second, three categories of challenges in using Lean Software Development were identified: ‘achieving flow’, ‘transparency’ and ‘creating a learning culture}
}
@article{Rodrguez2014CombiningLT,
	title        = {Combining Lean Thinking and Agile Methods for Software Development: A Case Study of a Finnish Provider of Wireless Embedded Systems Detailed},
	author       = {Pilar Rodr{\'i}guez and Jari Partanen and Pasi Kuvaja and Markku Oivo},
	year         = 2014,
	journal      = {2014 47th Hawaii International Conference on System Sciences},
	pages        = {4770--4779},
	abstract     = {Lean Software Development has attracted a great deal of attention during last years. However, it remains unclear how Lean is implemented in a domain that fundamentally differs from the automotive industry in which it originated. This study provides empirical evidence of how Lean can be combined with Agile methods to enhance software development processes. A case study was conducted at Elektrobit Wireless Segment, which has used Agile from 2007 and began to adopt Lean in 2010. Our findings evidence numerous compatibilities between Lean and Agile. In addition to well-established practices in Agile, Lean thinking has brought new elements to software development such as Kanban and work-in-progress limits, a “pull” and “less waste” oriented culture, and a stronger emphasis on transparency and collaborative development. Scaling flexibility, business management involvement and waste reduction were found as challenges, whilst setting up teams, self-organization and empowerment appeared easier to achieve.}
}
@article{MAHDAVIHEZAVEHI2013320,
	title        = {Variability in quality attributes of service-based software systems: A systematic literature review},
	author       = {Sara Mahdavi-Hezavehi and Matthias Galster and Paris Avgeriou},
	year         = 2013,
	journal      = {Information and Software Technology},
	volume       = 55,
	number       = 2,
	pages        = {320--343},
	doi          = {https://doi.org/10.1016/j.infsof.2012.08.010},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584912001772},
	note         = {Special Section: Component-Based Software Engineering (CBSE), 2011},
	keywords     = {Variability, Service-based systems, Quality attributes, Systematic literature review},
	abstract     = {Context Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.}
}
@article{5210118,
	title        = {A Systematic Review of the Application and Empirical Investigation of Search-Based Test Case Generation},
	author       = {Ali, Shaukat and Briand, Lionel C. and Hemmati, Hadi and Panesar-Walawege, Rajwinder Kaur},
	year         = 2010,
	month        = {Nov},
	journal      = {IEEE Transactions on Software Engineering},
	volume       = 36,
	number       = 6,
	pages        = {742--762},
	doi          = {10.1109/TSE.2009.52},
	issn         = {1939-3520},
	abstract     = {Metaheuristic search techniques have been extensively used to automate the process of generating test cases, and thus providing solutions for a more cost-effective testing process. This approach to test automation, often coined “Search-based Software Testing” (SBST), has been used for a wide variety of test case generation purposes. Since SBST techniques are heuristic by nature, they must be empirically investigated in terms of how costly and effective they are at reaching their test objectives and whether they scale up to realistic development artifacts. However, approaches to empirically study SBST techniques have shown wide variation in the literature. This paper presents the results of a systematic, comprehensive review that aims at characterizing how empirical studies have been designed to investigate SBST cost-effectiveness and what empirical evidence is available in the literature regarding SBST cost-effectiveness and scalability. We also provide a framework that drives the data collection process of this systematic review and can be the starting point of guidelines on how SBST techniques can be empirically assessed. The intent is to aid future researchers doing empirical studies in SBST by providing an unbiased view of the body of empirical evidence and by guiding them in performing well-designed and executed empirical studies.},
	keywords     = {Evolutionary computing and genetic algorithms, frameworks, heuristics design, review and evaluation, test generation, testing strategies, validation.}
}
@inproceedings{10.1145/1159789.1159792,
	title        = {Testing Telecoms Software with Quviq QuickCheck},
	author       = {Arts, Thomas and Hughes, John and Johansson, Joakim and Wiger, Ulf},
	year         = 2006,
	booktitle    = {Proceedings of the 2006 ACM SIGPLAN  Workshop on Erlang},
	location     = {Portland, Oregon, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ERLANG '06},
	pages        = {2–10},
	doi          = {10.1145/1159789.1159792},
	isbn         = 1595934901,
	url          = {https://doi.org/10.1145/1159789.1159792},
	abstract     = {We present a case study in which a novel testing tool, Quviq QuickCheck, is used to test an industrial implementation of the Megaco protocol. We considered positive and negative testing and we used our developed specification to test an old version in order to estimate how useful QuickCheck could potentially be when used early in development.The results of the case study indicate that, by using Quviq QuickCheck, we would have been able to detect faults early in the development.We detected faults that had not been detected by other testing techniques. We found unclarities in the specifications and potential faults when the software is used in a different setting. The results are considered promising enough to Ericsson that they are investing in an even larger case study, this time from the beginning of the development of a new product.},
	numpages     = 9,
	keywords     = {test automation, property based testing}
}
@article{BARNES2002272,
	title        = {Effective University â Industry Interaction:: A Multi-case Evaluation of Collaborative R&D Projects},
	author       = {Tina Barnes and Ian Pashby and Anne Gibbons},
	year         = 2002,
	journal      = {European Management Journal},
	volume       = 20,
	number       = 3,
	pages        = {272--285},
	doi          = {https://doi.org/10.1016/S0263-2373(02)00044-0},
	issn         = {0263-2373},
	url          = {https://www.sciencedirect.com/science/article/pii/S0263237302000440},
	keywords     = {University â Industry, Collaborative Research, Cultural Gap, Case Studies, Success Factors, Project Management, Partners, Framework, Practitioner},
	abstract     = {There is a growing world-wide trend toward greater collaboration between academia and industry, an activity encouraged by governments as a means of enhancing national competitiveness and wealth creation. Warwick Manufacturing Group (WMG) is well known for its extensive links with industry, and provided an excellent opportunity for a study of management practice within universityâindustry collaborative research projects. This paper evaluates the findings of six collaborative research projects. The objective was to identify factors which, if managed correctly, increase the probability of a collaboration being perceived as successful by both academic and industrial partners. The outcome was a good practice model for successful universityâindustry research collaborations.}
}
@article{Gorschek2005RequirementsAM,
	title        = {Requirements Abstraction Model},
	author       = {Tony Gorschek and Claes Wohlin},
	year         = 2005,
	journal      = {Requirements Engineering},
	volume       = 11,
	pages        = {79--101},
	abstract     = {Software requirements arrive in different shapes and forms to development organizations. This is particularly the case in market-driven requirements engineering, where the requirements are on products rather than directed towards projects. This results in challenges related to making different requirements comparable. In particular, this situation was identified in a collaborative effort between academia and industry. A model, with four abstraction levels, was developed as a response to the industrial need. The model allows for placement of requirements on different levels and supports abstraction or break down of requirements to make them comparable to each other. The model was successfully validated in several steps at a company. The results from the industrial validation point to the usefulness of the model. The model will allow companies to ensure comparability between requirements, and hence it generates important input to activities such as prioritization and packaging of requirements before launching a development project}
}
@article{Tremblay2010FocusGF,
	title        = {Focus Groups for Artifact Refinement and Evaluation in Design Research},
	author       = {Monica Chiarini Tremblay and Alan R. Hevner and Donald J. Berndt},
	year         = 2010,
	journal      = {Commun. Assoc. Inf. Syst.},
	volume       = 26,
	pages        = 27,
	abstract     = {Focus groups to investigate new ideas are widely used in many research fields. The use of focus groups in design research poses interesting opportunities and challenges. Traditional focus group methods must be adapted to meet two specific goals of design research. For the refinement of an artifact design, exploratory focus groups (EFGs) study the artifact to propose improvements in the design. The cycle of build and evaluate using EFGs continues until the artifact is released for field test in the application environment. Then, the field test of the design artifact may employ confirmatory focus groups (CFGs) to establish the utility of the artifact in field use. Rigorous investigation of the artifact requires multiple CFGs to be run with opportunities for quantitative and qualitative data collection and analyses across the multiple CFGs. In this paper, we discuss the adaptation of focus groups to design research projects. We demonstrate the use of both EFGs and CFGs in a design research project in the health care field.}
}
@inproceedings{493439,
	title        = {The role of experimentation in software engineering: past, current, and future},
	author       = {Basili, V.R.},
	year         = 1996,
	month        = {March},
	booktitle    = {Proceedings of IEEE 18th International Conference on Software Engineering},
	volume       = {},
	number       = {},
	pages        = {442--449},
	doi          = {10.1109/ICSE.1996.493439},
	issn         = {0270-5257},
	abstract     = {Software engineering needs to follow the model of other physical sciences and develop an experimental paradigm for the field. This paper proposes the approach towards developing an experimental component of such a paradigm. The approach is based upon a quality improvement paradigm that addresses the role of experimentation and process improvement in the content of industrial development. The paper outlines a classification scheme for characterizing such experiments.},
	keywords     = {}
}
@article{LEE1996843,
	title        = {Technology transfer and the research university: a search for the boundaries of university-industry collaboration},
	author       = {Yong S. Lee},
	year         = 1996,
	journal      = {Research Policy},
	volume       = 25,
	number       = 6,
	pages        = {843--863},
	doi          = {https://doi.org/10.1016/0048-7333(95)00857-8},
	issn         = {0048-7333},
	url          = {https://www.sciencedirect.com/science/article/pii/0048733395008578},
	abstract     = {This article examines the emerging âtechnology transferâ role US academics are expected to play in economic development, what specific roles they believe they can play in industrial innovations, and how they might go about collaborating with private industry. Based on a national survey response of approximately 1000 faculty members at research-intensive universities it concludes that US academics in the 1990s believe that they are more favorably disposed than in the 1980s toward closer university-industry collaboration. A majority of the respondents supports the idea that their universities participate actively in local and regional economic development, facilitate commercialization of academic research, and encourage faculty consulting for private firms. A majority of these respondents, however, refuses to support the idea of their universities getting involved in close business partnership with private industry by way of, for example, start-up assistance or equity investment. Of various organizational and motivational underpinnings analyzed from the data, two factors stand out as central to the current debate on university transfer: one is the perception of declining federal R&D support, which threatens the vitality of their research enterprise, and the other is the impact of close university-industry cooperation, which is likely to interfere with academic freedom â the freedom to pursue long-term, disinterested, fundamental research. A search for the boundaries of university-industry collaboration is, therefore, seen as a balancing act between these two competing concerns.}
}
@article{Lee2000TheSO,
	title        = {The Sustainability of University-Industry Research Collaboration: An Empirical Assessment},
	author       = {Yong Sung Lee},
	year         = 2000,
	journal      = {The Journal of Technology Transfer},
	volume       = 25,
	pages        = {111--133},
	abstract     = {The concept of university-industry collaboration is an important social experiment in the nation's innovation system. This study examines the sustainability of this collaborative experience by focusing on the actual “give-and-take” outcomes between university faculty members and industrial firms. Based on two separate but similar surveys conducted in 1997, one for faculty members and another for industry technology managers, the study reports that participants in research collaboration appear to realize significant benefits, some expected and others unexpected. The most significant benefit realized by firms is an increased access to new university research and discoveries, and the most significant benefits by faculty members is complementing their own academic research by securing funds for graduate students and lab equipment, and by seeking insights into their own research. Reflecting on their collaborative experience, an overwhelming majority of these participants say that in the future they would expand or at least sustain the present level of collaboration.}
}
